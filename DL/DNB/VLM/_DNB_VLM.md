
Vision Datasets and Benchmarks
Vision and Language Datasets
VLM
MLLM
CV

# Список

## Captioning / im2text

CC
LIAON
MS-COCO
COCO-Caption

Flickr30k
Nocaps
[[Visual Genome (VG)]]

[[_DNB_Image-Text]]

## Bench

[[MMStar]]
[[MME]]
[[MMMU]]
[[MMBench]]
[[MathVista]]
[[SeedBench]]

Ferret-Bench
LLaVA-Bench

## Training

LLaVA-Instruct-150K
ShareGPT4V dataset
LAION-GPT-V
LLaVAR

## Grounding / Reffering
RefCOCO
RefCOCO+
RefCOCOg

GRIT

## Остальное
[[POPE]]
MM-Vet
MMB / MMC
VizWiz

## VQA

**DONE**
[[VQA]]
[[VQAv2]]
[[DocVQA]]
[[ChartQA]]
[[PDFVQA]]
[[MP-DocVQA]]
[[AI2D]]
[[ScienceQA]]
[[GQA]]


**TODO**
InfographicVQA
[[ST-VQA]]
[[TextVQA]]
OKVQA
OCR-VQA

# Ссылки

**Papers references multiple datasets and benchmarks**
LLaVA-1.5
DeepSeek

# Обзор

Общая тенденция для новых бенчмарков показать что современные VLM работают плохо.

# Табличка

**Категории**
для тренировки / для оценки / both
interleaved text and images
image caption
text
OCR

**Столбцы для таблички**
Размер
Описание задачи
Использование ChatGPT
Как делать эвал
Сервер
Трейн/Тест
Формат

# Краткие обзоры

**LLaVA-Instruct-150K**
**LLaVA-Bench**
leverage GPT-4 to measure the quality of generated responses

**MS-COCO**
dataset containing around 120,000 images and 5-way image-caption annotations (produced by paid annotators).
vs. COCO-Caption ?

**REBUS**
REBUS: A Robust Evaluation Benchmark of Understanding Symbols
[https://arxiv.org/abs/2401.05604](https://arxiv.org/abs/2401.05604)

**AlgoPuzzleVQA**
[https://algopuzzlevqa.github.io/](https://algopuzzlevqa.github.io/)

**VisualWebBench**
[https://visualwebbench.github.io/](https://visualwebbench.github.io/)