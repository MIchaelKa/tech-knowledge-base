
# Внешние ссылки

**Paper**
Chameleon: Mixed-Modal Early-Fusion Foundation Models
https://arxiv.org/abs/2405.09818

**LN**
https://www.linkedin.com/posts/aiatmeta_new-paper-from-fair-chameleon-mixed-modal-activity-7197028924793528320-qV6L

> While some LLMs have separate image and text encoders or decoders, this research presents a family of early-fusion token-based mixed-modal models capable of understanding & generating images & text in any arbitrary sequence.

**Обзор**
https://artgor.medium.com/paper-review-chameleon-mixed-modal-early-fusion-foundation-models-580eddb0b21b

# Сводка

FAIR, Meta

**Основные моменты**

**Количество параметров**

**Оценка**

**Данные для тренировки**


# Ссылки


# Обзор