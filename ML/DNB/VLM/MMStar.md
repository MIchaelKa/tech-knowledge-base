
**Paper**
Are We on the Right Way for Evaluating Large Vision-Language Models?
https://arxiv.org/abs/2403.20330

https://mmstar-benchmark.github.io/


# Сводка

**Основные моменты**
Отсутствие визуальной зависимости
Лики в данных
Фильтрация существующих датасетов
Метрики для датасетов: MG / ML

**Формат**
Выбор из нескольких вариантов ответа.

**Размер**
1,500 примеров задач

# Ссылки

Sphinx
Sphinx-X-MoE


# Термины


# Вопросы

**MG vs. LLM Hit Rate**
Чем отличаются эти метрики?
Если смысл этих метрик похож, почему разные датасеты ведут себя по разному?
На примере MMMU
MMMU в целом сложный датасет, поэтому LLM Hit Rate у него низкий, но и с добавлением картинок метрика не увеличивается сильно.


# Обзор

**Метрики для датасетов.**
Переход от оценки моделей к оценке датасетов.

**Лики в данных**
Существуют лики в датасетах для тренировки и оценки VLM.

**Отсутствие визуальной зависимости**
Ненужность визуальной составляющей
Для многих примеров можно правильно ответить на вопрос не принимая во внимание визуальный вход вообще.
Когда составлялись датасеты на подобие [[AI2D]] не предполагалось что они будут решаться с помощью LLM обладающими большой памятью и знанием фактов.
Вопросы к составителям MMMU

**Фильтрация существующих датасетов**
Взяли за основу сущетсвующие датасеты
- MMBench
- MathVista
- SeedBench
- AI2D
- ScienceQA
- MMMU

**MG**
multi-modal gain
считаем метрику подавая картинку на вход и не подавая, затем считаем разницу
чем больше эта разница тем лучше для датасета
$MG = VLM_{img} - VLM_{text}$

MMBench - лучший на MG с большим отрывом
MMMU - худший

Почему MMStar не лучший?
Сложные вопросы в самом MMStar и сложно набрать такую разницу.


**ML**
multi-modal leakage
похожа на MG, разница между $VLM_{text}$ и базовой LLM
$ML = VLM_{text} - LLM$
чем больше эта разница тем хуже для датасета

MMStar - лучший

**LLM Hit Rate**
AI2D и ScienceQA - худший результат

# Данные


# Архитектура


# Обучение


# Оценка

