
Direct Preference Optimization: Your Language Model is Secretly a Reward Model
https://arxiv.org/abs/2305.18290

Simplifying Alignment: From RLHF to Direct Preference Optimization (DPO)
https://huggingface.co/blog/ariG23498/rlhf-to-dpo

chatgpt
https://chatgpt.com/c/68bb103e-3650-8330-a1c3-d95bb2a299b4

DPO avoids RL entirely
Вот почему говорят что по настоящему RL заработал только начиная с thinking models and R1