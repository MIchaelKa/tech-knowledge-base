

SIM-CoT: впервые латентный ризонинг догнал явный CoT
- https://t.me/abstractDL/348
- Классический коллапс пространства эмбеддингов.
- Решение — добавить step-level supervision.

SIM-CoT: Supervised Implicit Chain-of-Thought
- https://arxiv.org/abs/2509.20317
- Implicit Chain-of-Thought (CoT) methods

chatgpt
- https://chatgpt.com/c/68e0e914-0398-832f-b551-47fdc6ad29e7

Вопросы

- Как auxiliary decoder влияет на основную модель?
- Общая функция потерь
- Градиенты из L_step​ текут через вспомогательный декодер к латентам z_k​, и далее к параметрам основной модели.

- Что если объединить auxiliary decode и основную модели?
- ??

- Как градиенты проходят через латенты, ведь они в свою очередь не являются параметрами модели?
- сравнение с тем как проходит лосс в [[Variational Autoencoder (VAE)]]

- Stop-gradient (detach) — обрывают путь градиента ниже определённого уровня. Тогда L_step влияет только на слои после этого уровня.

- ранние слои (эмбеддинги, низкие блоки) получают градиенты, но они по норме часто на порядки меньше
- я прав что это также и верно при обычном обучении трансформера, а не только для сигнала который проходит через латенты в случае SIM-CoT