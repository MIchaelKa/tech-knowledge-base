
[[Attention]]
[[DeepSeek]]
[[RoPE]]
[[KV-Cache]]
[[Transformer]]

Matrix decomposition
- low-rank approximation
- rank factorization
- [[Singular Value Decomposition (SVD)]]

Paper
- The core of MLA is the low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference
- Dimension per head vs. Embedding dimension
- ùëä^ùëàùêæ, ùëä^ùëàùëâ up-projection matrices
- matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE)

A Gentle Introduction to Multi-Head Latent Attention (MLA)
- https://machinelearningmastery.com/a-gentle-introduction-to-multi-head-latent-attention-mla/
- Another key technique is that the multiple heads of attention are implemented only in the decompression matrices
- Looks like the formulas here are more complicated than in chatgpt explanation
- 

DeepSeek-V3 Technical Report
- https://t.me/gonzo_ML/3292
- –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π Multi-Head Attention (MHA)
	- –Ω–∞—Ä–µ–∑–∞—é—Ç—Å—è –Ω–∞ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
		- –Ω–∞—Ä–µ–∑–∞—é—Ç—Å—è ?
- —Å–æ–∫—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ KV-–∫–µ—à–∞, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞–¥–æ –∫–µ—à–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∏–∑–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ c_t, –∞ –Ω–µ –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ k_t, v_t –∫–∞–∫ —Ä–∞–Ω—å—à–µ
	- –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç? –≤—Å–µ –µ—â–µ –Ω—É–∂–Ω–æ –∑–∞—Ç–µ–º —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å c_t –≤ k_t, v_t?
- –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü
	- –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–æ–µ–∫—Ü–∏–π –∏–∑ c_t –≤ –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –≤–æ–æ–±—â–µ —É–±—Ä–∞—Ç—å
	- –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è k_t (W^uk) –º–æ–∂–Ω–æ –∏–Ω–∫–æ—Ä–ø–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–Ω—É—Ç—Ä—å –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è q_t (W^q)
	- –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è v_t (W^uv) –≤–Ω—É—Ç—Ä—å –≤—ã—Ö–æ–¥–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã W^o.
	- –∫–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç? –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—É—á–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å—Ä–∞–∑—É –¥–≤–µ –∑–∞–¥–∞—á–∏?
- decoupled RoPE
	- [[RoPE]]
	- –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ RoPE –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–æ–π –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π KV


Understanding Multi-Head Latent Attention
- https://planetbanatt.net/articles/mla.html
- rank factorization
- You can think of low-rank decomposition as trading memory costs for computation costs
	- At the same time KV-cache is trading computation costs for memory costs
- Perturbations in Low-Rank Decomposition
	- you are only able to travel from rank¬†r¬†matrix to rank¬†r¬†matrix


# chatgpt

- https://chatgpt.com/c/68ca81af-0b54-832b-b0b0-874bcf806f92
- Interactions with RoPE (Rotary Position Embeddings)
- DeepSeek-V2 introduces Partial-RoPE
- Compression is a learned projection
- Retrofitting


Questions

- Decompressing + applying RoPE every time you attend to past tokens would be very expensive.
- Don't we decompress keys and values for all past tokens when we predict next token anyway?
- Latent keys are never fully decompressed per token.
- Instead, the attention computation is reformulated so that the query interacts directly with the latent keys

- You only project the current query into the latent space.
- You never decompress all the old cached keys.
- That means we calculate the attention in the latent space. Is it true? Why previously you told about decompressing k and v to the full size using W_decomp?

- Parenthesis
- Whether you compute attention in latent space or full space depends on how you parenthesize the low-rank factorization.

- What about values?
- Previously there were the following statements
	- values need a decompression step ‚Äî because you actually need the weighted sum over value vectors in the model‚Äôs full dimension d.
	- But that‚Äôs just one matrix multiply after the softmax, not per past token per step.
- What the difference? We need to decompress all the values here, right? And it should require the same amount of computation as to reconstruct all the keys.
- Values are weighted with the attention matrix in the latent space too, then we decompress only the final weighted value.

Partial-RoPE

- Divide key on two parts: apply RoPE for the first part, compress the second part without the RoPE

- Am I right that compressed part is much bigger than the part where we apply the RoPE? Or otherwise compression would be no so efficient?

- I still cannot get why we cannot apply RoPE in the latent space? Why it less meaningful?

- Why not define a new RoPE in latent space?
- It wouldn‚Äôt correspond to the RoPE that the model was pre-trained with (which lives in full space).
- I want to say that we do not use RoPE to train from scratch? But what if we will? Can we in that case apply RoPE in latent space?

Heads

- K and V are shared between the heads or between the queries?
- Heads

- Am I right that in MLA each head uses the same K and V?
- Does each head have its own decompression matrix, or is it shared?
- Compression matrices are shared.
- Decompression matrices can be per head.
- See *A Gentle Introduction to Multi-Head Latent Attention (MLA)*