
# External Links

Учебник по машинному обучению
Яндекс
https://education.yandex.ru/handbook/ml

# Timeline

- 13.12.2023 - 14.12.2023
- 31.08.2024

# 2. Классический ML

# 2.1 Линейные модели

Что делать если признаки категориальные?
- one-hot encoding
- n - number of categories
- n-1 new columns

Feature engineering
- Дополнительные признаки, являющиеся сложными функциями от исходных

Интерпретируемость линейных моделей
- Плюс
- Нужно принимать во внимание Feature engineering

Линейная зависимость между признаками
- Модель может не сойтись
- Регуляризация

## Линейная регрессия

[[Linear Regression]]

Сведение к задаче оптимизации
[[Least Squares]]

Функция потерь
- loss function
- гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно

Функционал
- Функция потерь - функционал
- Функционал принимает на вход функцию (в данном случае это наша модель)

Функции потерь
- MSE
	- Mean Squared Error
	- Среднеквадратичное отклонение
	- МНК?
	- Normal noise
- MAE
	- Более устойчив к выбросам
- MAPE


### МНК - Точный аналитический метод

[[Covariance Matrix]]
[[SVD]]

Геометрический подход

Почему линейно-зависимые столбцы в матрице $Х$ приводят к не полному рангу в матрице $X^TX$ ?
Ранги этих матриц равны, но почему?

Обусловленность
- Плохо-обусловленная матрица, что это значит?
- эллипсоиды уровня функции потерь
- ru.wikipedia.org/wiki/Число_обусловленности
- TBD

### МНК - Приближенный численный метод

Минимизируемый функционал является гладким и выпуклым
- Как это доказать?

Градиентный спуск

Стохастический градиентный спуск
- матожидание оценки градиента на батче равно самому градиенту

стратегии отбора объектов
- чаще брать объекты, на которых ошибка больше.

LARS


## Регуляризация

[[Regularization]]

Единственность решения
- В случае если среди столбцов есть линейно зависимые, вектор весов может быть любой длинны, так как найдется вектор весов который будет давай ноль при умножении на любой элемент из выборки.
- Прибавляем вектор весов выше к векторы решения - получаем бесконечное множество различных решений
- Решение может быть сколько угодно большое по модулю.
- В случае линейной независимости столбцов есть только единственное решение?

Large weight values and overfitting
- Почему большие веса матрицы W это плохо?
- Малые погрешности признаков сильно возрастают при предсказании ответа

Система линейных уравнений
- Что означает решить систему линейных уравнений?
- Что означает если есть решение для системы где в правой части стоит нулевой вектор?
- TBD

Причины использовать регуляризацию
- Борьба с коллинеарными признаками и вырожденной матрицей весов
- Переобучение

Свойства
Уменьшает дисперсию, но решение становится смещенным (увеличивается байес)

L2 реуляризация
- ridge regression

L1 регуляризация
- Разряживание весов
- Обнуление определенных признаков

## Логистическая регрессия

[[Logistic Regression]]

Почему не решать как задачу регрессии?
- [[MSE for classification task]]
- MSE лосс почти не штрафует за объекты которые лежат близко к разделяющей плоскости, но не с той стороны

Логистическая регрессия
*Ещё один интересный метод появляется из желания посмотреть на классификацию как на задачу предсказания вероятностей.*

## Многоклассовая классификация

[[Classification]]

Один против всех (one-versus-all)
- Недостаток
	- несколько моделей обучаются на датасетах с разным распределением, выходы моделей могут иметь разные масштабы.

Все против всех (all-versus-all)
- $С_k^2$ - классификаторов
- алгоритмы могут не прийти к единому мнению (не будет класса который наберет максимальное количество голосов)

Sigmoin → Softmax
[[Cross Entropy Loss (CEL)]]

Линейный слой
one-versus-all + Softmax


## Масштабируемость

[[Scalability]]

SGD позволяет обучению хорошо масштабироваться по числу объектов

Как масштабироваться в случае большого числа признаков?
- Примеры когда это необходимо
	- мешок слов
	- tf-idf
- Решение
	- разряженное кодирование
	- hashing trick
	- vowpal wabbit
	- шардированная хеш-таблица