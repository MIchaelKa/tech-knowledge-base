
Deep Generative Models
Stefano Ermon

# External

- https://deepgenerativemodels.github.io/
- https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8
	- 2023

CS236 homeworks
- https://github.com/Subhajit135/CS236_DGM
- https://deepgenerativemodels.github.io/assignments.html

# Links

[[Generative Models]]

[[Bayesian Statistics]]

[[Probability Theory]]

# Notes


# Lecture 1 - Introduction

- Computer Vision as inverse task of image generation
- Statistical generative models
	- Model is a probability distribution
	- Dataset - samples from that distribution
	- After we train the model we can sample from that distribution
	- Have we access to the scalar probability value?
- Examples of using
	- Outlier detection
	- Super-resolution
	- Text2Speech
	- Machine translation as generative problem
- Control signal
	- text
	- low resolution
	- greyscale image
- Inverse problems
	- ?
- Challenges
	- Representation
		- How to model the joint dist. of many r.v. (ex.: different pixels if an image) ? 
	- Learning
		- How to compare probability distributions?


# Lecture 2 - Background

- Model family
	- Set of all possible distributions 
- Two questions
	- How to represent probability distribution?
	- How to learn them?
		- Need to define some notion of distance between two distributions
		- Loss
- Challenges
	- [[Curse of Dimensionality]]
- Bernoulli distribution
	- Only one parameter
	- We can learn it and generate new data
	- Is it possible to pose classification problem?
- Categorical
- Example of joint distribution
	- Modeling a color of a single pixel value
	- RGB channels
	- How many parameters we need?
		- $255^3 -1$ - if all channels values depends on each other
		- $255*3$ - if we can assume that channels are independent
- Example of joint distribution
	- black and white images
	- state space is $2^n$, where n is the number of pixels
	- sampling from $p(x_1, ... x_n)$ will generate an image
- Assumptions to deal with large state space
	- Links
		- [[Sum Rule]]
	- Independence
		- Join distribution can be factored as a product of marginal distributions
		- Parameters: n
- Two important rules
	- Chain Rule
	- Bayes' Rule
		- [[Bayes’ Theorem]]
		- Why it's important for application together with chain rule?
-  Chain Rule
	- Autoregressive models
	- Language modeling
	- [[Joint Probability]]
	- Parameters: $1 + 2 + ... + 2^{n-1}$
	- We still did not make any assumptions
	- Conditional independence
		- Markov chain assumption
		- Parameters: $2n - 1$
		- Big savings, much more reasonable model
		- BOW
- Valid PD
	- TODO
- Bayes Network
	- Conditionals a little bit more complex than when we just use chain rule
	- Condition on a set o variable
	- Bayesian Network
	- Not related to Neural Networks
	- Tabular representation
- Bayesian Network
	- Data structure to specify a probability distribution
	- DAG
	- Number of nodes equal to number of rv
	- You can change expressiveness of the network between markovian (1 parent for node) and fully defined joint distribution
	- vs. [[Probabilistic Graphical Model (PGM)]]
	- implies conditional independence
- generative vs. discriminative
	- discriminative
		- What we want to model  is still complex distribution $p(Y, X_1, ... X_n)$ with $2^{n-1}$ parameters
	- generative
		- you can predict anything from anything
		- impute missing things
		- more complex task
		- you need to model distribution between features as well
- Naive Bayes
	- generative model for discriminative task
	- [[Naive Bayes]]
	- Naive Bayes for single label prediction
	- All models are wrong, but some of them are usefull
- Logistic regression
	- intractable to have the full table with 2^n rows (parameters) for each possible combination of inputs, parameters in this table is probability of the label Y (100% or 0% ?)
	- assuming that there is some simple function
		- linear dependence assumption
	- Q: Is Logistic regression implies some independence assumption?
		- No
	- vs. Naive Bayes 
		- assuming less than Naive Bayes about how your variables are related, good if you have a lot of data
		- if you have less data we can try to use Naive Bayes
			- prior helps you more
- Neural Network
	- non-linear dependence
- Continuous variables
	- [[Normal Distribution]]
	- Mixture of two Gaussians
		- one rv is Bernulli
	

# Lecture 3 - Autoregressive Models

- Density estimation
	- p(x)
	- useful by itself
	- provides you a way to train a model using MLE
- How to represent probability distribution?
	- $p(x_1, ... x_{728})$
	- Chain Rule == Autoregressive Models
- Autoregressive Models
	- Ordering
		- Not clear which one to use for images
		- Pick an ordering of all rv
		- Example
			- raster scan ordering
	- Context
		- Length of context
	- RNN
	- Transformer
- FVSBN
	- Fully Visible Sigmoid Belief Networks (FVSBN)
	- FVBN
	- chain rule
	- LR
		- n-1 logistic regression problems
		- more and more features for LR
	- How to evaluate?
		- Product of LR functions
	- How to sample?
		- sample first pixel
		- use chain rule
	- Pros
		- simple technique
	- Cons
		- sequential 
	- How many parameters?
		- $1+2+3+...+n = n^2 / 2$
- NADE
	- Neural Autoregressive Density Estimation
	- Tie weights
		- reduce the number of parameters
		- speed up computation
			- how?
			- O(nd)
		- what assumption we make tying weights?
- Common
	- Conditional generation
	- Conditional generation vs. Modeling joint distribution
	- Label is just one more feature for generative models, but this feature can have much bigger influence
- RNADE
	- Mixture of K gaussians
	- vs. just one parameter as we have in linear regression ???
	- https://chatgpt.com/c/6751fca9-e6fc-8000-8c00-51c708f0c9c3
- Autoregressive models vs. Autoencoders
	- Can we get a generative model from AE?
	- Enforce some ordering
- MADE
	- Germain et al., 2015
	- Masked Autoencoder for Distribution Estimation
	- Ordering can be anything
	- Only training time benefit
	- Inference is still sequential
- RNN
	- [[RNN]]
	- No Markov assumption
	- [[Markov Chain]]
	- Cons
		- Sequential evaluation, cannot be parallelized
			- TODO
			- Cannot pack the sequence in one batch as transformer can.
			- Can we still can compose a batch from different samples?
- PixelRNN
	- Masking as in MADE
		- TODO
- PixelCNN
- Adversarial attacks and Anomaly detection
	- PixelDefend
		- Song et al., 2018
		- How to get p(x) from PixelCNN?
- Autoregressive Models
	- Pros
		- Easy to evaluate likelihood
			- Density estimation
			- How to get p(x) from PixelCNN?
	- Cons
		- No natural way to get features
- Links
	- How to get p(x) from PixelCNN?
		- https://chatgpt.com/c/674f0fca-7614-8000-902c-740781a42cbc

# Lecture 4 - Maximum Likelihood Learning

Learning a distribution
- we have samples only for most probable points in the space and don't have any data for less probable
- we want the best approximation of our data distribution, but what is the best mean?
- joint distribution of images and text
	- CLIP?
	- how we can imagine text distribution, where each text sequence has different length?
		- pad
		- chain rule

How should we measure distance between distributions?
- Autoregressive models
	- We can use likelihood since we have access to it
	- KL-Divergence
		- we care about compression
			- it may be not what you want
		- two options
			- E_{x~p_{data}}
				- only dataset is matter
			- E_{x~p_{model}}
				- reverse KL
				- I want the low loss on the things I generate myself
				- But how to compare them with p_data, if we don't know it outside of the dataset?
			- precision vs. recall
				- recall - cover all the dataset
				- how good is that we cover
		- minimizing KL-Div == maximizing expected log-likelihood
			- approximate expected log-likelihood with empirical log-likelihood
		- cons
			- we don't know how close we are to the optimum
				- we don't know the constant term
			- мы можем сравнить две модели, но не знаем насколько хороши мы в принципе

Training
- Regularization as reducing model family set
- If you have two model with similar training loss, prefer smaller one


# Lecture 5,6 - VAEs

11:45

Latent variable models

[[Variational Autoencoder (VAE)]]

Motivation
- Features: gender, hair color, etc., as factors variability
	- We do not have access to it (unless it explicitly annotated)
	- It can be useful for building a distribution.
	- Separate models for conditioning on these features
	- Using latent z variable is like clustering (by color, gender, etc.)
- If we had trained this model, then we could identify features via p(z | x)
	- Should we able to compute p(x | z) for it?
	- https://chatgpt.com/c/6752c69f-4660-8000-9ba9-3e9abfde395f

Mixture of Gaussians
- Shallow latent variable model
- Marginal can be very flexible/complex despite it consist of simple gaussians

VAE
- mixture of infinite gaussians, z~N(0,I)
- Can finite number of gaussians works better?
	- VQ?
- Even though p(x | z) is simple, the marginal p(x) is very complex

Evaluating likelihood
- First attempt: Naive Monte-Carlo
	- Sampling Z from uniform distribution is a bad solution
	- What stops us from sampling z from  ... ?
- Second attempt: Importance Sampling
	- gpt
		- https://chatgpt.com/c/67531880-bcb0-8000-a3f2-644d609365d9
		- https://chatgpt.com/c/6753381b-e784-8000-86ef-8962994dc0e4
	- If we directly take the logarithm of the Monte Carlo estimate the result becomes biased.
	- expectation of the log - true training objection
	- The Monte Carlo estimate is unbiased, but its log is not.
	- [[Jensen’s Inequality]]
	- $log(E[f(x,z)])$ vs. $E[log(f(x,z))]$
	- only one sample!
		- because we have only one sample we end up with expectation of the log

# LAST