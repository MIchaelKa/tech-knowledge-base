
Deep Generative Models
Stefano Ermon

# External

- https://deepgenerativemodels.github.io/
- https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8
	- 2023

CS236 homeworks
- https://github.com/Subhajit135/CS236_DGM
	- CS236 Fall 2018, The "[IAN](https://twitter.com/goodfellow_ian?lang=en)" class of Stanford.
- https://deepgenerativemodels.github.io/assignments.html

# Links

[[Generative Models]]

[[Energy-Based Models (EBMs)]]
[[Langevin MCMC]]

[[Bayesian Statistics]]

[[Calculus]]
- [[Integration by parts]]
- [[Jacobian]]
- Directional derivatives

[[Probability Theory]]
- Mixture of Gaussians

Proofs and derivations
- ELBO
- Contrastive divergence
- Score matching

# Papers

Search: Paper

Implicit Generation and Generalization in Energy-Based Models
- https://arxiv.org/abs/1903.08689
- Yilun Du, Igor Mordatch

How to Train Your Energy-Based Models
- https://arxiv.org/abs/2101.03288
- Yang Song, Diederik P. Kingma

Denoising score matching
- Vincent, 2011
- A Connection Between Score Matching and Denoising Autoencoders?

# Notes


# Lecture 1 - Introduction

- Computer Vision as inverse task of image generation
- Statistical generative models
	- Model is a probability distribution
	- Dataset - samples from that distribution
	- After we train the model we can sample from that distribution
	- Have we access to the scalar probability value?
- Examples of using
	- Outlier detection
	- Super-resolution
	- Text2Speech
	- Machine translation as generative problem
- Control signal
	- text
	- low resolution
	- greyscale image
- Inverse problems
	- ?
- Challenges
	- Representation
		- How to model the joint dist. of many r.v. (ex.: different pixels if an image) ? 
	- Learning
		- How to compare probability distributions?


# Lecture 2 - Background

Unsupervised learning
- [[Unsupervised Learning (UL)]]
- Unsupervised representation learning
- Unsupervised learning: recover high-level descriptions (features) from raw data

How to generate new images if know the distribution p(x)? Calculating p(x) with randomly generated samples until we find one with high probability? It obviously would not work.

- Model family
	- Set of all possible distributions 
- Two questions
	- How to represent probability distribution?
	- How to learn them?
		- Need to define some notion of distance between two distributions
		- Loss
- Challenges
	- [[Curse of Dimensionality]]
- Bernoulli distribution
	- Only one parameter
	- We can learn it and generate new data
	- Is it possible to pose classification problem?
		- https://chatgpt.com/c/6934150a-1fc4-8327-b034-77eefb849dcc
		- only when we have conditional distribution on some features
		- classification normally requires features
- Categorical
	- ?
- Example of joint distribution
	- Modeling a color of a single pixel value
	- RGB channels
	- How many parameters we need?
		- $255^3 -1$ - if all channels values depends on each other
		- $255*3$ - if we can assume that channels are independent
- Example of joint distribution
	- black and white images
	- state space is $2^n$, where n is the number of pixels
	- sampling from $p(x_1, ... x_n)$ will generate an image
- Assumptions to deal with large state space
	- Links
		- [[Probability Rules]]
	- Independence
		- Join distribution can be factored as a product of marginal distributions
		- Parameters: n
- Two important rules
	- Chain Rule
	- Bayes' Rule
		- [[Bayes’ Theorem]]
		- Why it's important for application together with chain rule?
-  Chain Rule
	- Autoregressive models
	- Language modeling
	- [[Joint Probability]]
	- Parameters: $1 + 2 + ... + 2^{n-1}$
	- We still did not make any assumptions
- Conditional independence
	- Markov chain assumption
	- Parameters: $2n - 1$
	- Big savings, much more reasonable model
	- BOW
- Valid PD
	- TODO
- Bayes Network
	- Generalization of *Conditional independence* idea
	- Conditionals a little bit more complex than when we just use chain rule
	- Condition on a set o variable
	- vs. Bayesian Network?
	- Not related to Neural Networks
	- Tabular representation 
- Bayesian Network
	- Data structure to specify a probability distribution
	- DAG
	- Number of nodes equal to number of r.v.
	- You can change expressiveness of the network between markovian (1 parent for node) and fully defined joint distribution
	- vs. [[Probabilistic Graphical Model (PGM)]]
	- implies conditional independence
- generative vs. discriminative
	- discriminative
		- What we want to model  is still complex distribution $p(Y, X_1, ... X_n)$ with $2^{n-1}$ parameters
			- ?
	- generative
		- you can predict anything from anything
		- impute missing things
		- more complex task
		- you need to mo  del distribution between features as well
- [[Naive Bayes]]
	- generative model for discriminative task
	- Naive Bayes for single label prediction
	- All models are wrong, but some of them are usefull
- Logistic regression
	- intractable to have the full table with 2^n rows (parameters) for each possible combination of inputs, parameters in this table is probability of the label Y (100% or 0% ?)
	- assuming that there is some simple function
		- linear dependence assumption
	- Q: Is Logistic regression implies some independence assumption?
		- No
	- vs. Naive Bayes 
		- assuming less than Naive Bayes about how your variables are related, good if you have a lot of data
		- if you have less data we can try to use Naive Bayes
			- prior helps you more
- Neural Network
	- non-linear dependence
	- replace all the conditionals we do not know how to deal with with neural net
- Continuous variables
	- [[Normal Distribution]]
	- Mixture of two Gaussians
		- one rv is Bernulli
	

# Lecture 3 - Autoregressive Models

- Density estimation
	- p(x)
	- useful by itself
	- provides you a way to train a model using MLE
- How to represent probability distribution?
	- $p(x_1, ... x_{728})$
	- Chain Rule == Autoregressive Models
- Autoregressive Models
	- Ordering
		- Not clear which one to use for images
		- Pick an ordering of all rv
		- Example
			- raster scan ordering
	- Context
		- Length of context
	- RNN
	- Transformer
- FVSBN
	- Fully Visible Sigmoid Belief Networks (FVSBN)
	- FVBN
	- chain rule
	- LR
		- n-1 logistic regression problems
		- more and more features for LR
	- How to evaluate?
		- Product of LR functions
	- How to sample?
		- sample first pixel
		- use chain rule
	- Pros
		- simple technique
	- Cons
		- sequential 
	- How many parameters?
		- $1+2+3+...+n = n^2 / 2$
- NADE
	- Neural Autoregressive Density Estimation
	- Tie weights
		- reduce the number of parameters
		- speed up computation
			- how?
			- O(nd)
		- what assumption we make tying weights?
- Common
	- Conditional generation
	- Conditional generation vs. Modeling joint distribution
	- Label is just one more feature for generative models, but this feature can have much bigger influence
- RNADE
	- Mixture of K gaussians
	- vs. just one parameter as we have in linear regression ???
	- https://chatgpt.com/c/6751fca9-e6fc-8000-8c00-51c708f0c9c3
- Autoregressive models vs. Autoencoders
	- Can we get a generative model from AE?
	- Enforce some ordering
- MADE
	- Germain et al., 2015
	- Masked Autoencoder for Distribution Estimation
	- Ordering can be anything
	- Only training time benefit
	- Inference is still sequential
- RNN
	- [[RNN]]
	- No Markov assumption
	- [[Markov Chain]]
	- Cons
		- Sequential evaluation, cannot be parallelized
			- TODO
			- Cannot pack the sequence in one batch as transformer can.
			- Can we still can compose a batch from different samples?
- PixelRNN
	- Masking as in MADE
		- TODO
- PixelCNN
- Adversarial attacks and Anomaly detection
	- PixelDefend
		- Song et al., 2018
		- How to get p(x) from PixelCNN?
- Autoregressive Models
	- Pros
		- Easy to evaluate likelihood
			- Density estimation
			- How to get p(x) from PixelCNN?
	- Cons
		- No natural way to get features
- Links
	- How to get p(x) from PixelCNN?
		- https://chatgpt.com/c/674f0fca-7614-8000-902c-740781a42cbc

# Lecture 4 - MLE

Maximum Likelihood Learning

Learning a distribution
- we have samples only for most probable points in the space and don't have any data for less probable
- we want the best approximation of our data distribution, but what is the best mean?
- joint distribution of images and text
	- CLIP?
	- how we can imagine text distribution, where each text sequence has different length?
		- pad
		- chain rule

Learning as density estimation
- regression problem?

How should we measure distance between distributions?
- Different ways to measure distance - different generative models
- Using KL-divergence is which type?
	- Autoregressive
		- Natural way for autoregressive models since we have access to the likelihood.
- One possible way to measure a similarity between distributions: if you are not able to distinguish between samples generated from two distributions
	- GAN

Data compression
- MLE is based on compression
- When you know the data distribution you also know how to compress the data
	- The KL-Divergence tells you how well the scheme for q will work on p
	- Number of extra bits you will need on average
- D (p||q)
	- The price is log(p(x)/q(x))
	- The price is weighted by the probability of p
	- Expectation for all x
- KL-Divergence
	- we care about compression
	- it may be not what you want
- Morse code

Forward vs. Reverse KL
- Forward KL: E_{x~p_{data}}
	- only dataset is matter
	- interesting that for Autoregressive models we want Forward FL but for VAE we want Reverse
- Reverse KL: E_{x~p_{model}}
	- I want the low loss on the things I generate myself
	- But how to compare them with p_data, if we don't know it outside of the dataset?
	- What is possible is mode seeking behaviour, when model will put all the probability mass on a single point and KLD will be perfect on that exactly point
- precision vs. recall
	- recall - cover all the dataset
	- precision -  how good is that we cover

Forward KL

$p(x) = p_{data}$
$q(x) = p_{model} = p_{\theta}$

$KL(p||q) = {E}_{x \sim p(x)}\left[\log\frac{p(x)}{q(x)}\right]$


Expected log-likelihood
- Forward KLD formula may look challenging since it depends on something we even cannot compute - $p_{data}(x)$
- Decompose, then see that the first term does not depend on parameters of your model (theta)
	- constant term
- Expectation over data distribution
	- still cannot optimize over it, we need access to $p_{data}$ for that since this is part of expectation formula
	- empirical log-likelihood
- Monte-Carlo estimation
	- [[Law of Large Numbers (LLN)]]

What is connection between KLD and MLE?
- minimizing KLD == maximizing expected log-likelihood
	- approximate expected log-likelihood with empirical log-likelihood

Cons
- we don't know how close we are to the optimum
	- we don't know the constant term
- мы можем сравнить две модели, но не знаем насколько хороши мы в принципе

Training
- Regularization as reducing model family set
- If you have two model with similar training loss, prefer smaller one


# Lecture 5,6 - VAEs

11:45

Latent variable models

[[Variational Autoencoder (VAE)]]

Motivation
- Features: gender, hair color, etc., as factors variability
	- We do not have access to it (unless it explicitly annotated)
	- It can be useful for building a distribution.
	- Separate models for conditioning on these features
	- Using latent z variable is like clustering (by color, gender, etc.)
- If we had trained this model, then we could identify features via p(z | x)
	- Should we compute p(x | z) for it?
	- https://chatgpt.com/c/6752c69f-4660-8000-9ba9-3e9abfde395f

Mixture of Gaussians
- Shallow latent variable model
- Marginal can be very flexible/complex despite it consist of simple gaussians

VAE
- Mixture of infinite gaussians, z~N(0,I)
- Can finite number of gaussians works better?
	- VQ?
- Even though p(x | z) is simple, the marginal p(x) is very complex

Evaluating likelihood
- First attempt: Naive Monte-Carlo
	- Sampling Z from uniform distribution is a bad solution
	- What stops us from sampling z from  ... ?
- Second attempt: [[Importance Sampling]]
	- gpt
		- https://chatgpt.com/c/67531880-bcb0-8000-a3f2-644d609365d9
		- https://chatgpt.com/c/6753381b-e784-8000-86ef-8962994dc0e4
	- If we directly take the logarithm of the Monte Carlo estimate the result becomes biased.
	- expectation of the log - true training objection
		- this is not right ???
	- The Monte Carlo estimate is unbiased, but its log is not.
	- [[Jensen’s Inequality]]
	- $log(E[f(x,z)])$ vs. $E[log(f(x,z))]$
	- only one sample!
		- because we have only one sample we end up with expectation of the log


Unsupervised Learning
- Clustering and latent z
- extension of k-means
- We can use latent z for clustering objects
- Example: all people with the same eyes color in one cluster

Laten Z
- Can we have more complex distribution for it, instead of using standard normal?
- Learn standard distribution parameters
- Predict z with another VAE
	- Hierarchical / StackedVAEs
	- Diffusion models - DM

How to build more powerful models
- More flexible models
- Stacking VAEs vs. Adding layers vs. Adding dimensions to z


Variational Inference
- E step in [[EM Algorithm]]
- Learning model when you have missing data

Log-likelihood as if fully observed
- https://chatgpt.com/c/6754873a-9b3c-8000-b6b4-b213a4c3adb3

Intractable posterior
- Can we use a mixture of gaussians for approximating p(z | x), instead of modeling it by q which is parameterized to define just one gaussian
	- Answering here https://youtu.be/8cO61e_8oPY?si=NiXqVcsxU_wbks2r&t=2230
	- Using autoregressive model?

Optimization
- Optimizing family of lower bounds
- Family because of q distribution, different q gives us different lower bound
- M and E steps in [[EM Algorithm]]

Inference
- Density estimation - use q
- Generation - use p

ELBO for entire dataset
- Different q parameters for each datapoint
- Example when we modeling top half of the image by having independent parameter for each pixel
- But what if we have neural net which can infer needed top half given bottom?
	- This is what is called Amortized Inference

Optimization 2
- step on q params
- step on p params
- joint optimization?

Gradients for p params
- Slide 13
- How we get L2 or cross-entropy loss from optimizing log(p(x,z)) ?
	- TODO
	- https://youtu.be/m6dKKRsZwBQ?si=nW_GmZ-Pw1j3QqII&t=1118
- Exactly same gradient as in autoregressive models...
- Maximum probability of the train image at the mean of gaussian distribution where this mean is equal to this image.

Gradients for q params
- Reinforce
	- [[Reinforcement Learning (RL)]]
	- Used in VQ?
- Reparameterization trick
	- less general
	- only works when latent z is continuous
	- rv does not depend on q params anymore
- How to push gradient through Expectation
	- when gradient and expectation depends on the same variable
	- grad E -> E grad

VAE
- AE + defining prior distribution for z = generative model = VAE


# Lecture 7,8 - Normalizing Flows


Links
- Normalizing Flows Tutorial
- https://blog.evjang.com/2018/01/nf1.html

Points
- Another way to going around intractable marginal probability in a latent variable model.
- How to design a latent variable model with tractable likelihood?
- In VAE it's hard to get p(x)?
	- Using q(z|x)
- You can think of diffusion models as certain kind of flow models
	- infinitely deep?
	- diffusion models (DM) unify flow models and VAE

Flow models
- Key idea: Map simple distribution to complex using invertible transformations
- What if we can easily invert p(x|z) and construct p(z|x)
	- Make x = f(z) deterministic invertible function
	- For any x we have unique z
- Flow models is VAE when mapping from z to x is deterministic and invertible
- Now x and z have the same number of dimensions
	- Why we have such restriction?
	- If it's not, the mapping would not be invertible.
		- Why?
		- For every x will be several corresponding z
- Generation process is deterministic
	- Limitation?
	- How to get different samples from one z in VAE?
		- z is computed using unit variance and zero mean

Background
- Change of variables formula
- Random vectors
	- Linear case: matrix and determinant
	- Non-linear case: determinant of Jacobian
		- How volume is stretched locally

Normalizing Flow
- Flow of transformations
- Note: determinant of product is equal to product of determinants
- Stack of neural layers
	- Should we handle non-linear layers somehow?

Flow models intuition
- It's hard to model some complicated dataset using simple n-D gaussian
- It becomes possible after applying several transformations to the original density function
- Find PMF that put more probability mass to the datapoints that you have in your training set

Learning and inference
- Learning
	- x -> z
	- Exact likelihood evaluation using inverse transformation
- Inference
	- z -> x
	- Forward transformation

Jacobian matrix
- n x n,  where n is the data dimensionality
- pretty large matrix, need to compute determinant efficiently
- Choose transformation so that Jacobian has special structure
	- For example triangular matrix -> we can find determinant in O(n)

Triangular Jacobian
- f_i(z) depends only on z <= i
- similar to autoregressive models
	- masked AE?

NICE
- Nonlinear Independent Components Estimation
- (Dinh et al., 2014)
- Volume preserving transformation (determinant is 1)
- only shift

Real-NVP
- Non-volume preserving extension of NICE
- shift + scale

Autoregressive models as Flow models
- Continuos Gaussian autoregressive models


MAF
- Masked Autoregressive Flow
- Connection with MADE
- training is efficient
- generation is sequential and slow

IAF
- Inverse Autoregressive Flow
- generation is parallel and efficient
- training is slow
	- but can efficiently calculate likelihood for generated samples, this property is used in Parallel Wavenet

Parallel Wavenet
- Paper
	- Parallel WaveNet: Fast High-Fidelity Speech Synthesis
	- https://arxiv.org/abs/1711.10433
	- Google
- Speech Synthesis
- MAF + IAF + Teacher-Student Distillation
- Probability Density Distillation
- D_KL(s, t)
	- important to sample from s model for monte-carlo estimation of above this D_KL
- Q:
	- Can we do the same trick for language model?
	- No, because X is discrete?
		- TODO
	- But ok for images?
		- 0 - 255 is also is discrete space but we can add noise, and previously it was mentioned that this is not the problem for images
		- See: Reparameterization trick
	- Do we have the requirement of continuous data for Flow model?
		- Yes, only applicable to PDF not PMF


MintNet
- Is it possible to define invertible layers that is convolutional?
- Masked convolutions
- vs. PixelCNN


Gaussianization Flow
- Inverse CDF Trick
- [[Histogram Equalization]]
- TODO

# Lecture 9,10 - GANs

[[Generative Adversarial Networks (GAN)]]

MLE
- All previous models
- log-likelihood
- D_KL ==> MLE
	- Show this
	- TODO
- We don't need to know the true data distribution to calculate D_KL(P_data || P_model) ?
	- We use data samples to estimate the expectation
	- We can only optimize D_KL up to the shift!
- How to compare probability distributions in a different way?
- Why MLE?
	- Optimal statistical efficiency
	- Compression and likelihood

MLE free learning
- Do we really want MLE for generating good images?
	- For imperfect models high log-likelihood does not always mean good sample quality and vice versa (Theis et al., 2016)
 - Negative augmentation
	 - Tricky to do with likelihood models
 - Case 2
	 - Great test log-likelihood, poor samples.
	 - Is having one perfect image from 100 is really bad?
	 - Use ensembles to deal with 0.99 of noise from model
 - Case 3
	 - Great samples, poor log-likelihood

Distance measure
- Try to change distance measure for comparing two distribution, do not use D_KL
- Comparing distribution via samples
	- [[Two-sample test]]
	- hand-crafted statistics
	- comparing mean of two groups
	- likelihood free
- Key idea: Learn statistic
	- Discriminator
	- Learn Two-sample statistic
	- Test statistic: -loss of the classifier
	- Classification problem
	- Again likelihood?
		- But this classifier is also use cross-entropy or D_KL.
		- We use likelihood of the classifier, not generative model.
		- Likelihood over binary variable, not input image x

Generator
- Looks like a flow model, but the mapping shouldn't be invertible

Generator architecture restrictions
- https://chatgpt.com/c/675d86f1-8008-8000-ab6f-f5fe0b5856e7
- GAN
	- Now we don't any restrictions
- Autoregressive
	- Sequential process of generation
	- masked convolutions
	- masked attention
- Normalizing Flow
	- Invertible
	- Same number of dimensions for z and x
- VAE
	- Which restrictions VAE have?
	- Latent variable

Loss
- Jensen-Shannon Divergence (JSD)
	- D_KL between distribution and mixture of two distributions ? x2
	- Symmetric KL Divergence
- Under ideal conditions (optimal discriminator) this is equivalent to choosing for d(p_data, p_model) JSD instead of KLD


Problems
- Unstable optimization
	- Do not know when to stop, as with MLE when you can see that loss no longer improves
	- Why?
- Mode collapse
	- Focus on a few types of data points (in training set?) and completely stop generating others
	- few types of data points - they called modes
	- How to protect generator from memorizing the training set?
		- It does not have the access to it!
- Evaluation


GAN
- likelihood-free training

## f-GAN

f-Divergences
- Broader class of divergences

Fenchel conjugate
- https://chatgpt.com/c/675da715-8e20-8000-bdec-ea26fcaae9e8
- convex conjugate
- best linear approximation
- duality
- [[Convex Optimization]]

supremum
- https://en.wikipedia.org/wiki/Infimum_and_supremum
- supremum = least upper bound

f-GAN
- minimizing lower bound of f-Divergence

Why it's better than DKL?
- DKL == Compression
- Might not be what we want

## Wasserstein GAN

Support of a distribution: set of points that have non-zero probability

Problems with f-Divergence
- Arises when p and q have disjoint supports
- No signal from training in this case since Divergence always equal to some constant

Wasserstein distance
- Earth-Mover distance

Lipschitz constant
- TODO
- Lipschitz constant of f(x) is 1


## Inferring latent representation

Solution 1
- Use features from discriminator

Solution 2
- How to use generator?
- BiGAN
	- Encoder is deterministic mapping unlike in VAE


# Lecture 11, 12 - Energy Based Models

[[Energy-Based Models (EBMs)]]

DM
- EBM is closely related to DM
- As well as NF and VAE

Models based on KLD
- Models
	- Autoregressive
		- Chain Rule
	- NF
		- ?
	- VAE
		- Approximation
- Cons
	- Model architectures is restricted.
		- We need to define a valid probability distribution.
		- See Lecture 2?

Models not based on KLD
- GAN

EBM
- allows flexible model architectures
- stable training
- likelihood based
- inspired by statistical physics
- expanding our "green set" which potentially will allow you to get closer to the target distribution
- very general set of models

Probability distribution p(x)
- Parameterizing probability distribution p(x)
- You cannot pick an arbitrary functions
	- Valid probability density p(x)
		- p(x) >= 0
		- integrate to 1
- 2 restrictions
	- non-negative
		- Easy to satisfy
	- integrate/sum to 1
		- Trickier to satisfy
		- Intuition why we need
			- Total volume is fixed, increasing one p(x_1) we are sure that probability will decrease for all other xs
		- How this property achieved in other models?
			- NF - clear
			- VAE - ?
			- Autoregressive
				- chain rule?
			- TODO
- Partition function - the integral of all space, all the volume
- Choose our function to be able to compute this partition function analytically
	- Example: [[Normal Distribution]]
	- Exponential family
	- [[Ветров. Введение в вероятностный язык построения моделей машинного обучения]]

Likelihood-based learning
- Autoregressive
- Latent Variables
	- Mixture of normalized objects
	- VAE
		- normalized objects is p(x|z)
		- mixture is p(x)
		- p(x|z) = N(x;g(x),var(x))
		- g(x),var(x) - decoder output
- Why it's normalized by construction?
	- DONE
- Why VAE not in this list?
	- Latent Variables?

EBM
- Why exponent? Not x^2 ?
	- Model big variations in probability and have smooth f(x). Exp is helping with modeling of big variations keeping f(x) smooth. Small change in f(x) – big changes in probability.
	- Working with log-prob
	- My question in [[dlcourse.ai]]
- vs. [[Softmax]]
	- For categorical distribution
	- Exactly same form
	- Softmax in VAE?
		- TODO
	- In the case of Softmax the Partition function can be computed analytically, exactly and easily
		- Integrate over all possible classes in possible
	- In the case of Softmax we are learning just p(y|x)

EBM
- pros
	- flexibility
		- success of diffusion models comes from here?
- cons
	- https://chatgpt.com/c/693eeb65-4a18-8328-8928-52b14716f11a
		- Y
		- EMB only tell you: “Some regions of space have lower energy (higher probability).” But they do not tell you how to jump there.
		- This creates the famous “negative phase” problem.
	- sampling is hard
		- even if you can train the model, generate samples will be tricky
			- why if we need to calculate the normalization constant only once (for a given set of parameters)?
		- there is no ordering
			- MCMC methods to looks for samples that are likely under the model
			- Langevin dynamics – A very common EBM sampler
		- invert CDF
			- ?
	- evaluation p(x) is hard, so hard learning
		- need to calculate the normalization constant
		- about learning is clear since during the training we always need to recalculate this normalization constant
	- In general case the partition function is very hard to compute, if we model p(x)
- [[Curse of Dimensionality]]

Application of EBM
- When some task do not requires partition function
- As long as required only relative comparison of p(x) and p(x')
- Compare two data samples
	- Classification
	- Anomaly Detection
	- Denoising

Ising Model
- old school approach to denoising
- https://chatgpt.com/c/67654fca-9eb4-8000-bddd-94f379a58b3d
- Ising Model in Physics
	- ferromagnetism
- Potentials
	- How energy and potential relates?
		- Energy = − (sum of potentials)
	- Is energy gradient of potentials function?
		- NO
- Denoising process
	- find y that maximizing p(y|x) or equivalently p(x,y)
	- normalization constant doesn't matter
	- how denoising process looks like, we need to try different y's for that right? but how to select those y's?
		- Gibbs Sampling (a type of MCMC)

Product of Experts
- product - AND
	- when you multiply different models which one of them is normalized, the product is not normalized
	- need normalization constant
- sum - OR - mixture

Restricted Boltzmann Machine (RBM)
- https://chatgpt.com/c/69406cdb-f85c-832f-be74-2e8ddb198cc0
- EMB with latent variables
- [[Quadratic Form]]
- How to compute normalization constant?
- Restricted
	- visible units are _not directly connected_.
	- Because of the restricted structure, each hidden unit is conditionally independent:
- Where is the input to this model?
	- You give the model `x` (image)
	- It produces a hidden representation `z` (features)
		- We can get p(z|x) from EMB formulation of p(x,z)
		- Recognize the sigmoid
- Why we need Z?
	- Modeling dependencies between visible units
	- Without `z`, you’d need direct $x_i x_j​$ interactions → intractable learning.
	- Hidden variables convert hard dependencies into easy conditional independence.
	- Easy p(z|x) and p(x|z) is the key
- How to sample?
	- Gibbs sampling
	- Below

Deep Boltzmann Machines
- Salakhutdinov and Hinton, 2009

## Training

Training intuition
- Just increasing unnormalized log-probability do not guarantee that this example becomes more likely
- We need to increase the probability of the train datapoint and decrease probability of everything else
- Idea: instead of evaluating $Z(\Theta)$ exactly we will use Monte Carlo estimate

Contrastive Divergence Algorithm
- This algorithm was used to train the RBM
- Monte-Carlo estimate for partition function using negative samples
- Negative samples
	- samples from the model
	- it can work initially when the models isn't well trained, but how it will work in later training steps?
	- it also comes form the Gradient of log-likelihood
- Maximizing log-likelihood
- Gradient of log-likelihood
- Monte-Carlo estimate with one sample from the model
	- How to sample?

## Sampling (MCMC)

How to get samples?
We don't have a direct way to do this as we have for example with autoregressive models.
We need samples during training as well, not only during inference

We cannot evaluate probability of a datapoint
It requires knowing the partition function.

What we can do is to compare two datapoints.

Markov Chain Monte Carlo (MCMC)
- Local search / optimization
- Use the fact we can do comparison
- Algorithm
	- Start with randomly initialized x
	- Add some noise
	- Compare two samples, which one is more likely under the model
	- Update x if the new version was more likely
		- accept
		- reject - we can also accept with some probability which depends on how bad the new version of the image are
			- exploration
	- Repeat
- Works in theory, in practice need a large number of steps to converge
- Convergence slows down exponentially with increasing dimensions
- The algorithm described here is it MCMC or more specific version MH-MCMC?

MCMC methods
- Langevin dynamics
- HMC
	- ?
- Metropolis-Hastings
	- MH-MCMC
	- MCMC with accept/reject - ?

---

MH-MCMC
- Recap
- Works in theory
	- Detailed balance condition
	- TODO
- In practice convergence is slow

Unadjusted Langevin MCMC
- [[Langevin MCMC]]
- Special case of above
- Noisy gradient ascent
	- Use gradient of the log-likelihood with respect to x
	- This gradient with respect to x is easy to compute
	- This gradient is called score function
- You do not need accept or reject
- Unadjusted
	- TODO
- Cons
	- Convergence is still slow
	- Reasonable for inference time
	- Very expensive training since this sampling is required on each step. For example in Contrastive Divergence algorithm.

Goal: Training without sampling
- Score matching
- Noise Contrastive Estimation
- Adversarial training

Contrastive Divergence is approximation to KLD
We need faster training

## Score matching

Score function (Stein)
- Gradient of log probability with respect to x, not theta
- Function of x, and theta
	- (see example with gaussian where theta is just two numbers - mean and variance)
	- parameterized by theta
- Why gradient of log?
	- log is monotonic, so gradient points to the same direction(and with the same magnitude) as for original PDF?
	- math is easy for EBM
- Vector field
	- The score is the gradient
	- Alternative interpretation of PDF as a vector field, instead of a scalar field
- Equivalent in Physics
	- energy function - potential
	- score function - electric field
- Computationally it might my more handy to use one vs. another
	- Potential is more handy in Physics!
		- A
	- Field is more handy in DL

Score matching
- Fisher divergence
	- Difference between gradients of two distributions
	- If two distribution p and q are similar, they also should have similar gradients
- Connection between Fisher divergence and KLD
	- Fisher divergence is kind of derivative
	- At the end of the day so far this is just another way to define how we calculate similarity between p_data and p_model
- We do not know the gradient of the true data distribution
	- [[Integration by parts]]
	- We need to assume that p_data decays rapidly when x -> +/- infinity
		-  x -> +/- infinity: what does it mean for the image?
- Univariate score matching
- Multivariate score matching
	- trace of the Hessian of log p_model
- Hessian is expensive, we will need to approximate it
	- Paper: Denoising score matching (Vincent 2010)
	- Paper: Sliced score matching  (Song 2019)
- Intuition of the loss function
	- Finding flat regions where gradient vectors are small
	- Hessian to ensure it's maximum, not minimum
	- TODO: 2nd derivative in multivariable calculus
	- TODO: Почему мы от квадрата переходит к квадрату L2 нормы в multivariable?
	- [[Laplacian]] / [[Calculus]]

Q
- When learning will we compute the gradient for gradient?
	- First - gradient wrt x
	- Second - gradient wrt theta, for updating params

Recap
- KL Divergence == MLE
	- Contrastive divergence
	- need to sample from model for training
- Fisher Divergence == Score matching
	- no need to sample


## NCE

Noise Contrastive Estimation (NCE)
- Instead of contrasting data samples to samples from the model, we are going to contrast samples to samples from some noise distribution
	- Similar to GANs
- Define discriminator in terms of generative models
	- Optimal discriminator which use EBM inside
	- Very specific way to define discriminator
- Optimal discriminator shod know p_data for making predictions
	- Can we apply the same reasoning for simple binary classifier?
	- TODO
- Partition function as a learnable parameter
	- Just a scaler?
	- Why we didn't do this earlier?
-  Noise distribution
	- Noise distribution is fixed
	- It theory it works regardless what is the noise distribution
	- In practice it depends on the quality of the noise distribution (how close it to the real data distribution)
	- Depending on the noise it can be very close to the score matching.
- Training vs. Inference
	- NCE is only about training, for the inference need to use one of the usual EBM procedures like MCMC, etc.

NCE vs. GAN

Flow Contrastive Estimation
- Use NF for generating noise as close to the true data distribution as we can to make work of discriminator harder
- Looks a lot like a GAN


# Lecture 13 - Score Based Models

Summary
- Dealing with Hessian?
	- YES
- More general than EBMs

Is score matching limited to EBMs?
- NO
- We can apply them to
	- continuous autoregressive models
	- NF

Model the score function
- What's the most general model we can train with score matching?
- There is a broader set of models then EBMs and that's the idea of Score based models.
- Instead of modeling the energy, we are directly model the score function (vector field of gradients).
- Defining our model by defining the corresponding field of gradients.
- Output of NN now is a gradient vector, not scalar?
- R^d -> R^d 

How it will change our loss previously obtained for EBM?
- It will change only the definition of the score function itself?
- The score function was already as general as possible no?
	- We have the special form for EBMs
- Remove the log?
	- There is no log in the final expression for the score of EBMs.
- Но мы хотим моделировать именно градиент логарифма, то что он исчезает говорит об определенной специфики модели

Do we still need to model a gradient of a log of a pdf?
- Why not just a gradient of a pdf?
- Why gradient of log? (see above)

Broader family of models.
- The gradient you are modeling might be not ...
	- the gradient of a scalar function
	- conservative vector field
		- Physics
- There might not be an underlying scalar function
- We are allowed to learn arbitrary vector field

Original function vs. gradient
- We looses the constant.
- For PDF this is not important since it should integrate to one
- TODO: 
	- How this is related to each other?
	- Why this shift can be determined?

How to compare two vector fields?
- Average Euclidian distance over all the space
- This is exactly the Fisher divergence

[[Jacobian]] -> [[Hessian]]

Score matching is not scalable
- Avoids partition function but still expensive
- Trace of the Jacobian is the hard part
- O(d) backprop, where d is the number of dimension
	- Why we need to do several different backprop passes?
	- What we usually get when we do autograd?
	- I think it's usual autograd but now we have d elements in the output.
	- But when we are making L2 for images in VAE we also have d elements in the output.
		- But the total loss have only one number
	- It's like train with d different losses.

## Denoising score matching

Denoising score matching
- Vincent, 2011
- Estimate score of the noised version of the distribution
	- Adding noise to the data or convolving data density with noise distribution
	- noise-perturbed data distribution
	- p_data(x) -> q(x')
	- q(x') - very complex
- Perturbation/distribution kernel, noise distributin
	- q(x'|x) - simple Gaussian
- loss derivation
	- compare loss derivation with vanilla version
		- TODO
- definition of q(x')
	- convolving vs. marginalization
	- TODO
	- see notebook
- Score matching reduces to denoising
	- What score model is trying to do is to estimate the noise that was added to x to produce x'
	- The noise will be equal to the gradient of the perturbed or original density wrt to x'?
	- If the original it makes total sense, but...
	- The noise will be equal to the gradient that estimated by the model
	- Model approximates perturbed! density
	- Make total sense for the perturbed density, see notes
	- Tweedie's formula!
- Stein unbiased risk estimator
	- ?
- Estimating derivatives
	- Numerical approximation
	- Estimating derivatives through a finite difference
	- Estimating derivatives through perturbation  
- Loss formula
	- Why plus?


- Pros
	- Scalable
	- Reduces score estimation to denoising
- Cons
	- Estimates scores of noise-perturbed data
		- How it affect the quality of samples?
		- We can generate only noisy samples! More about this later...
			- Really? - YES!

Q:


- Is the gradient of the disturbed image (distribution?) should be similar to the noise?
	- Yes
	- But why, if we estimates the gradient of the noise-perturbed data?
		- Make total sense for the perturbed density, see notes
	- Tweedie's formula


- Why we can't use the same model if we train it to predict the noise?
	- [chatgpt](https://chatgpt.com/g/g-p-676b182bc0e8819196b1f598b592448e-generative-models/c/676b1a2b-3ea8-8000-80ea-a24a636f3d09)
		- Revisited 19/12/25: Вопрос не корректный.
	- Related to: Cons - Estimates scores of noise-perturbed data
	- Получается что даже умея предсказывать тот шум который был добавлен мы не можем получить сэмпл из исходного распределения используя Langevin dynamic. 
	- Что если просто вычитать его из изображения?
		- Это тоже самое что и следовать градиенту
	- Ответ в том что в любом случае мы знаем только направление? Мы не учитываем длину вектора шума, когда обучаем модель?
	- Good example for understanding scores of noise-perturbed density
		- Good estimation when we far away from the original curve
		- Bad estimation when we close to it
		- https://youtu.be/E69Lp_T9nVg?si=G-9_jsgtW4pnVncb&t=1410
	- The solution to this is using multiple noise scales (Lecture 14)

## Sliced score matching

Sliced score matching
- Comparing projections of the vectors instead of comparing them fully
- Random projections
- Directional derivatives
- Backpropagation is now efficient
	- We still has [[Jacobian]]?
		- Why in 1D case?
		- Mini-batch of v's?
			- Rather no, we will have even in 1D
		- You need to understand the derivation and [[Integration by parts]]

- Pros
	- Scalable
	- Estimates true data density
- Cons
	- Slower than denoising score matching

## Sampling

Sampling
- [[Langevin MCMC]]
- Adding a bit of Gaussian Noise is a must
- Langevin dynamic sampling is valid MCMC procedure in the limit
	- Why we need to tell about it if even adding a noise is a valid MCMC procedure?

Cons
- The above vanilla procedure doesn't work!
	- Even with added noise during training?
- Langevin converges slowly

Pitfalls
- Data lies in manifolds
	- Score function might be not well defined
	- TODO?
- Challenges in low data density regions
	- Paper: Generative Modeling by Estimating Gradients of the Data Distribution
- Mixing of Langevin dynamic
	- Example with mixture of two distributions
	- Score function has no dependence on the mode weighing
		- TODO: mode
	- Modes of data distribution


# Lecture 14

Connection between score-based models and [[Diffusion Models]]
Diffusion model is the score matching of multiple levels of noise-perturbed data densities

The solution to all above pitfalls: Gaussian perturbation

How much noise to add?
- Tradeoff
	- Add as little noise as possible to estimate something close to the real data
	- The more noise you add the better estimation becomes

Solution: Using multiple noise scales
- Annealed Langevin dynamic
	- [[Langevin MCMC]]
- How many noise steps you need?
	- typical magic number is 1000
	- 1000 -> infinite many number of steps (continuous)

Joint score estimation
- You need to be able jointly estimate the amount of noise of different levels
- Noise conditional score networks (NCSN)
- Additional single input to neural net with:
	- the step number
	- sigma

Training
- Denoising score matching
- Sliced score matching?
	- for noise-perturbed data?
	- TODO

Loss
- jointly solve many problems for different noise levels
- weighting factor

Choosing noise scales
- max - maximum pairwise distance between datapoints
- min - samples should looks like clean images

Choosing the weighting factor
- How much should we care about different noise levels?
- In the vanilla implementation we would have scaled(weighted) version of the score function right?

DM vs. GAN
- Why DM are better? Is there any theoretical grounding around it?
- At inference time: What DM is doing is very similar to inference time scaling!
- At training time it's the opposite: only small incremental predictions with comparison to GANs that need to generate the whole image each time.

We are not yet at Diffusion Models!

## Infinite number of noise levels

The noise level 
- The noise level that you use during inference should match the noise level you used during training?
- Как мы можем перескакивать уровни во время инференса?
- Мы предсказываем уровень шума между двумя последующими уровнями или всегда до конечной, чистой картинки?
	- С одной стороны до конечной так как мы всегда получаем perturbed density из оригинального
	- C другой стороны во время инференса (Annealed Langevin dynamic) мы всегда остаемся в этом уровне шума
- chatgpt
	- https://chatgpt.com/c/6945e717-d314-8327-8478-deda32ece1b3
	- Skipping noise levels is possible when diffusion is interpreted as a continuous-time process.

Infinite number of noise levels
- See picture from the slide!
- Index t is now continuous number, not the discrete step
- What represent the path of the image on this picture?
	- Sequence of random variables each drawn from their own distribution
	- Stochastic process
	- Forward SDE
	- Random walk
	- Случайное блуждание
- SDE
	- Forward SDE
		- Toy SDE
		- WLOG - Without Loss Of Generality
		- No deterministic part
	- Reverse SDE
		- Score function
		- What this score function mean? Previously we had many score function for each noise level. Same thing here?
			- YES, now NCSN takes real number as a noise level
		- How to get the formula with Score function for Reverse SDE?
			- TODO
			- https://chatgpt.com/c/694bb364-740c-8325-87df-c4fd67e9e9ca
			- уравнение Фоккера–Планка
- Generation via reversing stochastic process
	- Similar on solving SDE by numerical methods
	- Paper: Score-Based Generative Modeling through Stochastic Differential Equations
	- Euler-Maruyama
		- Analogous to Euler method for ODE
		- The process is very similar to Langevin dynamic
	- Pros
		- You have a freedom to choose noise levels. You don't need to apply 1000 steps of denoising anymore
		- At the moment you managed to formulate the problem of sampling as solving SDE then you can use a lot of advanced numerical methods for solving SDE (for our task of image generation)
	- Solution
		- Solution to the Reverse SDE gives you path from pure noise to real images.
		- We are numerically integrating the Reverse SDE to get the solution that goes from noise to data
- Predictor-Corrector sampling
	- Numerical SDE solver
	- Corrector
		- Corrector is Langevin dynamic
	- Predictor
		- What is predictor?
		- Just tell you how far away you need to jump at time(steps) scale?
	- Есть ли какая-то математическая обоснованность что это оптимальный способ делать шаги в уровне шума?
- Training
	- Estimate the score function as we did before, but now it can take infinite number of values
- Discrete vs. Continuous
	- What is the difference with discrete levels of noise? Just use random real value from [0,1] for the noise level during training?
	- Predictor-Corrector sampling


Diffusion ODE
- https://chatgpt.com/c/694bb364-740c-8325-87df-c4fd67e9e9ca
- Converting SDE to ODE
	- Now the Forward ODE depends on the score
- Convert this model to NF
- ODE and NF
	- Mapping is invertible
	- TODO



# LAST