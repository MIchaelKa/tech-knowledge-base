
Deep Generative Models
Stefano Ermon

# External

- https://deepgenerativemodels.github.io/
- https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8
	- 2023

# Links

[[Generative Models]]

[[Bayesian Statistics]]

[[Probability Theory]]

# Notes


# Lecture 1 - Introduction

- Computer Vision as inverse task of image generation
- Statistical generative models
	- Model is a probability distribution
	- Dataset - samples from that distribution
	- After we train the model we can sample from that distribution
	- Have we access to the scalar probability value?
- Examples of using
	- Outlier detection
	- Super-resolution
	- Text2Speech
	- Machine translation as generative problem
- Control signal
	- text
	- low resolution
	- greyscale image
- Inverse problems
	- ?
- Challenges
	- Representation
		- How to model the joint dist. of many r.v. (ex.: different pixels if an image) ? 
	- Learning
		- How to compare probability distributions?


# Lecture 2 - Background

- Model family
	- Set of all possible distributions 
- Two questions
	- How to represent probability distribution?
	- How to learn them?
		- Need to define some notion of distance between two distributions
		- Loss
- Challenges
	- [[Curse of Dimensionality]]
- Bernoulli distribution
	- Only one parameter
	- We can learn it and generate new data
	- Is it possible to pose classification problem?
- Categorical
- Example of joint distribution
	- Modeling a color of a single pixel value
	- RGB channels
	- How many parameters we need?
		- $255^3 -1$ - if all channels values depends on each other
		- $255*3$ - if we can assume that channels are independent
- Example of joint distribution
	- black and white images
	- state space is $2^n$, where n is the number of pixels
	- sampling from $p(x_1, ... x_n)$ will generate an image
- Assumptions to deal with large state space
	- Links
		- [[Sum Rule]]
	- Independence
		- Join distribution can be factored as a product of marginal distributions
		- Parameters: n
- Two important rules
	- Chain Rule
	- Bayes' Rule
		- [[Bayesâ€™ Theorem]]
		- Why it's important for application together with chain rule?
-  Chain Rule
	- Autoregressive models
	- Language modeling
	- [[Joint Probability]]
	- Parameters: $1 + 2 + ... + 2^{n-1}$
	- We still did not make any assumptions
	- Conditional independence
		- Markov chain assumption
		- Parameters: $2n - 1$
		- Big savings, much more reasonable model
		- BOW
- Valid PD
	- TODO
- Bayes Network
	- Conditionals a little bit more complex than when we just use chain rule
	- Condition on a set o variable
	- Bayesian Network
	- Not related to Neural Networks
	- Tabular representation
- Bayesian Network
	- Data structure to specify a probability distribution
	- DAG
	- Number of nodes equal to number of rv
	- You can change expressiveness of the network between markovian (1 parent for node) and fully defined joint distribution
	- vs. [[Probabilistic Graphical Model (PGM)]]
	- implies conditional independence
- generative vs. discriminative
	- discriminative
		- What we want to model  is still complex distribution $p(Y, X_1, ... X_n)$ with $2^{n-1}$ parameters
	- generative
		- you can predict anything from anything
		- impute missing things
		- more complex task
		- you need to model distribution between features as well
- Naive Bayes
	- generative model for discriminative task
	- [[Naive Bayes]]
	- Naive Bayes for single label prediction
	- All models are wrong, but some of them are usefull
- Logistic regression
	- intractable to have the full table with 2^n rows (parameters) for each possible combination of inputs, parameters in this table is probability of the label Y (100% or 0% ?)
	- assuming that there is some simple function
		- linear dependence assumption
	- Q: Is Logistic regression implies some independence assumption?
		- No
	- vs. Naive Bayes 
		- assuming less than Naive Bayes about how your variables are related, good if you have a lot of data
		- if you have less data we can try to use Naive Bayes
			- prior helps you more
- Neural Network
	- non-linear dependence
- Continuous variables
	- [[Normal Distribution]]
	- Mixture of two Gaussians
		- one rv is Bernulli
	

# Lecture 3 - Autoregressive Models

- Density estimation
	- p(x)
	- useful by itself
	- provides you a way to train a model using MLE
- How to represent probability distribution?
	- $p(x_1, ... x_{728})$
	- Chain Rule == Autoregressive Models
- Autoregressive Models
	- Ordering
		- Not clear which one to use for images
		- Pick an ordering of all rv
		- Example
			- raster scan ordering
	- Context
		- Length of context
	- RNN
	- Transformer
- FVSBN
	- Fully Visible Sigmoid Belief Networks (FVSBN)
	- FVBN
	- chain rule
	- LR
		- n-1 logistic regression problems
		- more and more features for LR
	- How to evaluate?
		- Product of LR functions
	- How to sample?
		- sample first pixel
		- use chain rule
	- Pros
		- simple technique
	- Cons
		- sequential 
	- How many parameters?
		- $1+2+3+...+n = n^2 / 2$
- NADE
	- Neural Autoregressive Density Estimation
	- Tie weights
		- reduce the number of parameters
		- speed up computation
			- how?
			- O(nd)
		- what assumption we make tying weights?
- Common
	- Conditional generation
	- Conditional generation vs. Modeling joint distribution
	- Label is just one more feature for generative models, but this feature can have much bigger influence
- RNADE
	- Mixture of K gaussians
- Autoregressive models vs. Autoencoders
	- Can we get a generative model from AE?
	- Enforce some ordering
- MADE
	- Germain et al., 2015
	- Masked Autoencoder for Distribution Estimation
	- Ordering can be anything
	- Only training time benefit
	- Inference is still sequential
- RNN
	- [[RNN]]
	- No Markov assumption
	- [[Markov Chain]]
	- Cons
		- Sequential evaluation, cannot be parallelized
			- TODO
			- Cannot pack the sequence in one batch as transformer can.
			- Can we still can compose a batch from different samples?
- PixelRNN
	- Masking as in MADE
		- TODO
- PixelCNN
- Adversarial attacks and Anomaly detection
	- PixelDefend
		- Song et al., 2018
		- How to get p(x) from PixelCNN?
			- https://chatgpt.com/c/674f0fca-7614-8000-902c-740781a42cbc
- Autoregressive Models
	- Cons
		- No natural way to get features

# Lecture 4 - Maximum Likelihood Learning




# LAST