
Deep Generative Models
Stefano Ermon

# External

- https://deepgenerativemodels.github.io/
- https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8
	- 2023

CS236 homeworks
- https://github.com/Subhajit135/CS236_DGM
- https://deepgenerativemodels.github.io/assignments.html

# Links

[[Generative Models]]

[[Bayesian Statistics]]

[[Probability Theory]]

# Notes


# Lecture 1 - Introduction

- Computer Vision as inverse task of image generation
- Statistical generative models
	- Model is a probability distribution
	- Dataset - samples from that distribution
	- After we train the model we can sample from that distribution
	- Have we access to the scalar probability value?
- Examples of using
	- Outlier detection
	- Super-resolution
	- Text2Speech
	- Machine translation as generative problem
- Control signal
	- text
	- low resolution
	- greyscale image
- Inverse problems
	- ?
- Challenges
	- Representation
		- How to model the joint dist. of many r.v. (ex.: different pixels if an image) ? 
	- Learning
		- How to compare probability distributions?


# Lecture 2 - Background

Unsupervised learning
- [[Unsupervised Learning (UL)]]
- Unsupervised representation learning
- Unsupervised learning: recover high-level descriptions (features) from raw data

How to generate new images if know the distribution p(x)? Calculating p(x) with randomly generated samples until we find one with high probability? It obviously would not work.

- Model family
	- Set of all possible distributions 
- Two questions
	- How to represent probability distribution?
	- How to learn them?
		- Need to define some notion of distance between two distributions
		- Loss
- Challenges
	- [[Curse of Dimensionality]]
- Bernoulli distribution
	- Only one parameter
	- We can learn it and generate new data
	- Is it possible to pose classification problem?
		- https://chatgpt.com/c/6934150a-1fc4-8327-b034-77eefb849dcc
		- only when we have conditional distribution on some features
		- classification normally requires features
- Categorical
	- ?
- Example of joint distribution
	- Modeling a color of a single pixel value
	- RGB channels
	- How many parameters we need?
		- $255^3 -1$ - if all channels values depends on each other
		- $255*3$ - if we can assume that channels are independent
- Example of joint distribution
	- black and white images
	- state space is $2^n$, where n is the number of pixels
	- sampling from $p(x_1, ... x_n)$ will generate an image
- Assumptions to deal with large state space
	- Links
		- [[Probability Rules]]
	- Independence
		- Join distribution can be factored as a product of marginal distributions
		- Parameters: n
- Two important rules
	- Chain Rule
	- Bayes' Rule
		- [[Bayes’ Theorem]]
		- Why it's important for application together with chain rule?
-  Chain Rule
	- Autoregressive models
	- Language modeling
	- [[Joint Probability]]
	- Parameters: $1 + 2 + ... + 2^{n-1}$
	- We still did not make any assumptions
- Conditional independence
	- Markov chain assumption
	- Parameters: $2n - 1$
	- Big savings, much more reasonable model
	- BOW
- Valid PD
	- TODO
- Bayes Network
	- Generalization of *Conditional independence* idea
	- Conditionals a little bit more complex than when we just use chain rule
	- Condition on a set o variable
	- vs. Bayesian Network?
	- Not related to Neural Networks
	- Tabular representation 
- Bayesian Network
	- Data structure to specify a probability distribution
	- DAG
	- Number of nodes equal to number of r.v.
	- You can change expressiveness of the network between markovian (1 parent for node) and fully defined joint distribution
	- vs. [[Probabilistic Graphical Model (PGM)]]
	- implies conditional independence
- generative vs. discriminative
	- discriminative
		- What we want to model  is still complex distribution $p(Y, X_1, ... X_n)$ with $2^{n-1}$ parameters
			- ?
	- generative
		- you can predict anything from anything
		- impute missing things
		- more complex task
		- you need to mo  del distribution between features as well
- [[Naive Bayes]]
	- generative model for discriminative task
	- Naive Bayes for single label prediction
	- All models are wrong, but some of them are usefull
- Logistic regression
	- intractable to have the full table with 2^n rows (parameters) for each possible combination of inputs, parameters in this table is probability of the label Y (100% or 0% ?)
	- assuming that there is some simple function
		- linear dependence assumption
	- Q: Is Logistic regression implies some independence assumption?
		- No
	- vs. Naive Bayes 
		- assuming less than Naive Bayes about how your variables are related, good if you have a lot of data
		- if you have less data we can try to use Naive Bayes
			- prior helps you more
- Neural Network
	- non-linear dependence
	- replace all the conditionals we do not know how to deal with with neural net
- Continuous variables
	- [[Normal Distribution]]
	- Mixture of two Gaussians
		- one rv is Bernulli
	

# Lecture 3 - Autoregressive Models

- Density estimation
	- p(x)
	- useful by itself
	- provides you a way to train a model using MLE
- How to represent probability distribution?
	- $p(x_1, ... x_{728})$
	- Chain Rule == Autoregressive Models
- Autoregressive Models
	- Ordering
		- Not clear which one to use for images
		- Pick an ordering of all rv
		- Example
			- raster scan ordering
	- Context
		- Length of context
	- RNN
	- Transformer
- FVSBN
	- Fully Visible Sigmoid Belief Networks (FVSBN)
	- FVBN
	- chain rule
	- LR
		- n-1 logistic regression problems
		- more and more features for LR
	- How to evaluate?
		- Product of LR functions
	- How to sample?
		- sample first pixel
		- use chain rule
	- Pros
		- simple technique
	- Cons
		- sequential 
	- How many parameters?
		- $1+2+3+...+n = n^2 / 2$
- NADE
	- Neural Autoregressive Density Estimation
	- Tie weights
		- reduce the number of parameters
		- speed up computation
			- how?
			- O(nd)
		- what assumption we make tying weights?
- Common
	- Conditional generation
	- Conditional generation vs. Modeling joint distribution
	- Label is just one more feature for generative models, but this feature can have much bigger influence
- RNADE
	- Mixture of K gaussians
	- vs. just one parameter as we have in linear regression ???
	- https://chatgpt.com/c/6751fca9-e6fc-8000-8c00-51c708f0c9c3
- Autoregressive models vs. Autoencoders
	- Can we get a generative model from AE?
	- Enforce some ordering
- MADE
	- Germain et al., 2015
	- Masked Autoencoder for Distribution Estimation
	- Ordering can be anything
	- Only training time benefit
	- Inference is still sequential
- RNN
	- [[RNN]]
	- No Markov assumption
	- [[Markov Chain]]
	- Cons
		- Sequential evaluation, cannot be parallelized
			- TODO
			- Cannot pack the sequence in one batch as transformer can.
			- Can we still can compose a batch from different samples?
- PixelRNN
	- Masking as in MADE
		- TODO
- PixelCNN
- Adversarial attacks and Anomaly detection
	- PixelDefend
		- Song et al., 2018
		- How to get p(x) from PixelCNN?
- Autoregressive Models
	- Pros
		- Easy to evaluate likelihood
			- Density estimation
			- How to get p(x) from PixelCNN?
	- Cons
		- No natural way to get features
- Links
	- How to get p(x) from PixelCNN?
		- https://chatgpt.com/c/674f0fca-7614-8000-902c-740781a42cbc

# Lecture 4 - MLE

Maximum Likelihood Learning

Learning a distribution
- we have samples only for most probable points in the space and don't have any data for less probable
- we want the best approximation of our data distribution, but what is the best mean?
- joint distribution of images and text
	- CLIP?
	- how we can imagine text distribution, where each text sequence has different length?
		- pad
		- chain rule

How should we measure distance between distributions?
- Autoregressive models
	- We can use likelihood since we have access to it
	- KL-Divergence
		- we care about compression
			- it may be not what you want
		- two options
			- E_{x~p_{data}}
				- only dataset is matter
			- E_{x~p_{model}}
				- reverse KL
				- I want the low loss on the things I generate myself
				- But how to compare them with p_data, if we don't know it outside of the dataset?
			- precision vs. recall
				- recall - cover all the dataset
				- how good is that we cover
		- minimizing KL-Div == maximizing expected log-likelihood
			- approximate expected log-likelihood with empirical log-likelihood
		- cons
			- we don't know how close we are to the optimum
				- we don't know the constant term
			- мы можем сравнить две модели, но не знаем насколько хороши мы в принципе

Training
- Regularization as reducing model family set
- If you have two model with similar training loss, prefer smaller one


# Lecture 5,6 - VAEs

11:45

Latent variable models

[[Variational Autoencoder (VAE)]]

Motivation
- Features: gender, hair color, etc., as factors variability
	- We do not have access to it (unless it explicitly annotated)
	- It can be useful for building a distribution.
	- Separate models for conditioning on these features
	- Using latent z variable is like clustering (by color, gender, etc.)
- If we had trained this model, then we could identify features via p(z | x)
	- Should we able to compute p(x | z) for it?
	- https://chatgpt.com/c/6752c69f-4660-8000-9ba9-3e9abfde395f

Mixture of Gaussians
- Shallow latent variable model
- Marginal can be very flexible/complex despite it consist of simple gaussians

VAE
- Mixture of infinite gaussians, z~N(0,I)
- Can finite number of gaussians works better?
	- VQ?
- Even though p(x | z) is simple, the marginal p(x) is very complex

Evaluating likelihood
- First attempt: Naive Monte-Carlo
	- Sampling Z from uniform distribution is a bad solution
	- What stops us from sampling z from  ... ?
- Second attempt: Importance Sampling
	- gpt
		- https://chatgpt.com/c/67531880-bcb0-8000-a3f2-644d609365d9
		- https://chatgpt.com/c/6753381b-e784-8000-86ef-8962994dc0e4
	- If we directly take the logarithm of the Monte Carlo estimate the result becomes biased.
	- expectation of the log - true training objection
		- this is not right ???
	- The Monte Carlo estimate is unbiased, but its log is not.
	- [[Jensen’s Inequality]]
	- $log(E[f(x,z)])$ vs. $E[log(f(x,z))]$
	- only one sample!
		- because we have only one sample we end up with expectation of the log


Unsupervised Learning
- Clustering and latent z
- extension of k-means
- We can use latent z for clustering objects
- Example: all people with the same eyes color in one cluster

Laten Z
- Can we have more complex distribution for it, instead of using standart normal?
- Learn standart distribution parameters
- Predict z with another VAE
	- Hierarchical / StackedVAEs
	- Diffusion models - DM

How to build more powerful models
- More flexible models
- Stacking VAEs vs. Adding layers vs. Adding dimensions to z


Variational Inference
- E step in [[EM Algorithm]]
- Learning model when you have missing data

Loglikelihood as if fully observed
- https://chatgpt.com/c/6754873a-9b3c-8000-b6b4-b213a4c3adb3

Intractable posterior
- Can we use a mixture of gaussians for approximating p(z | x), instead of modeling it by q which is parameterized to define just one gaussian
	- Answering here https://youtu.be/8cO61e_8oPY?si=NiXqVcsxU_wbks2r&t=2230
	- Using autoregressive model?

Optimization
- Optimizing family of lower bounds
- Family because of q distribution, different q gives us different lower bound
- M and E steps in [[EM Algorithm]]

Inference
- Density estimation - use q
- Generation - use p

ELBO for entire dataset
- Different q parameters for each datapoint
- Example when we modeling top half of the image by having independent parameter for each pixel
- But what if we have neural net which can infer needed top half given bottom?
	- This is what is called Amortized Inference

Optimization 2
- step on q params
- step on p params
- joint optimization?

Gradients for p params
- Slide 13
- How we get L2 or cross-entropy loss from optimizing log(p(x,z)) ?
	- TODO
	- https://youtu.be/m6dKKRsZwBQ?si=nW_GmZ-Pw1j3QqII&t=1118
- Exactly same gradient as in autoregressive models...
- Maximum probability of the train image at the mean of gaussian distribution where this mean is equal to this image.

Gradients for q params
- Reinforce
	- [[Reinforcement Learning (RL)]]
	- Used in VQ?
- Reparameterization trick
	- less general
	- only works when latent z is continuous
	- rv does not depend on q params anymore
- How to push gradient through Expectation
	- when gradient and expectation depends on the same variable
	- grad E -> E grad

VAE
- AE + defining prior distribution for z = generative model = VAE


# Lecture 7,8 - Normalizing Flows


Links
- Normalizing Flows Tutorial
- https://blog.evjang.com/2018/01/nf1.html

Points
- Another way to going around intractable marginal probability in a latent variable model.
- How to design a latent variable model with tractable likelihood?
- In VAE it's hard to get p(x)?
	- Using q(z|x)
- You can think of diffusion models as certain kind of flow models
	- infinitely deep?
	- diffusion models (DM) unify flow models and VAE

Flow models
- Key idea: Map simple distribution to complex using invertible transformations
- What if we can easily invert p(x|z) and construct p(z|x)
	- Make x = f(z) deterministic invertible function
	- For any x we have unique z
- Flow models is VAE when mapping from z to x is deterministic and invertible
- Now x and z have the same number of dimensions
	- Why we have such restriction?
	- If it's not, the mapping would not be invertible.
		- Why?
		- For every x will be several corresponding z
- Generation process is deterministic
	- Limitation?
	- How to get different samples from one z in VAE?
		- z is computed using unit variance and zero mean

Background
- Change of variables formula
- Random vectors
	- Linear case: matrix and determinant
	- Non-linear case: determinant of Jacobian
		- How volume is stretched locally

Normalizing Flow
- Flow of transformations
- Note: determinant of product is equal to product of determinants
- Stack of neural layers
	- Should we handle non-linear layers somehow?

Flow models intuition
- It's hard to model some complicated dataset using simple n-D gaussian
- It becomes possible after applying several transformations to the original density function
- Find PMF that put more probability mass to the datapoints that you have in your training set

Learning and inference
- Learning
	- x -> z
	- Exact likelihood evaluation using inverse transformation
- Inference
	- z -> x
	- Forward transformation

Jacobian matrix
- n x n,  where n is the data dimensionality
- pretty large matrix, need to compute determinant efficiently
- Choose transformation so that Jacobian has special structure
	- For example triangular matrix -> we can find determinant in O(n)

Triangular Jacobian
- f_i(z) depends only on z <= i
- similar to autoregressive models
	- masked AE?

NICE
- Nonlinear Independent Components Estimation
- (Dinh et al., 2014)
- Volume preserving transformation (determinant is 1)
- only shift

Real-NVP
- Non-volume preserving extension of NICE
- shift + scale

Autoregressive models as Flow models
- Continuos Gaussian autoregressive models


MAF
- Masked Autoregressive Flow
- Connection with MADE
- training is efficient
- generation is sequential and slow

IAF
- Inverse Autoregressive Flow
- generation is parallel and efficient
- training is slow
	- but can efficiently calculate likelihood for generated samples, this property is used in Parallel Wavenet

Parallel Wavenet
- Paper
	- Parallel WaveNet: Fast High-Fidelity Speech Synthesis
	- https://arxiv.org/abs/1711.10433
	- Google
- Speech Synthesis
- MAF + IAF + Teacher-Student Distillation
- Probability Density Distillation
- D_KL(s, t)
	- important to sample from s model for monte-carlo estimation of above this D_KL
- Q:
	- Can we do the same trick for language model?
	- No, because X is discrete?
		- TODO
	- But ok for images?
		- 0 - 255 is also is discrete space but we can add noise, and previously it was mentioned that this is not the problem for images
		- See: Reparameterization trick
	- Do we have the requirement of continuous data for Flow model?
		- Yes, only applicable to PDF not PMF


MintNet
- Is it possible to define invertible layers that is convolutional?
- Masked convolutions
- vs. PixelCNN


Gaussianization Flow
- Inverse CDF Trick
- [[Histogram Equalization]]
- TODO

# Lecture 9,10 - GANs

[[Generative Adversarial Networks (GAN)]]

MLE
- All previous models
- log-likelihood
- D_KL ==> MLE
	- Show this
	- TODO
- We don't need to know the true data distribution to calculate D_KL(P_data || P_model) ?
	- We use data samples to estimate the expectation
	- We can only optimize D_KL up to the shift!
- How to compare probability distributions in a different way?
- Why MLE?
	- Optimal statistical efficiency
	- Compression and likelihood

MLE free learning
- Do we really want MLE for generating good images?
	- For imperfect models high log-likelihood does not always mean good sample quality and vice versa (Theis et al., 2016)
 - Negative augmentation
	 - Tricky to do with likelihood models
 - Case 2
	 - Great test log-likelihood, poor samples.
	 - Is having one perfect image from 100 is really bad?
	 - Use ensembles to deal with 0.99 of noise from model
 - Case 3
	 - Great samples, poor log-likelihood

Distance measure
- Try to change distance measure for comparing two distribution, do not use D_KL
- Comparing distribution via samples
	- [[Two-sample test]]
	- hand-crafted statistics
	- comparing mean of two groups
	- likelihood free
- Key idea: Learn statistic
	- Discriminator
	- Learn Two-sample statistic
	- Test statistic: -loss of the classifier
	- Classification problem
	- Again likelihood?
		- But this classifier is also use cross-entropy or D_KL.
		- We use likelihood of the classifier, not generative model.
		- Likelihood over binary variable, not input image x

Generator
- Looks like a flow model, but the mapping shouldn't be invertible

Generator architecture restrictions
- https://chatgpt.com/c/675d86f1-8008-8000-ab6f-f5fe0b5856e7
- GAN
	- Now we don't any restrictions
- Autoregressive
	- Sequential process of generation
	- masked convolutions
	- masked attention
- Normalizing Flow
	- Invertible
	- Same number of dimensions for z and x
- VAE
	- Which restrictions VAE have?
	- Latent variable

Loss
- Jensen-Shannon Divergence (JSD)
	- D_KL between distribution and mixture of two distributions ? x2
	- Symmetric KL Divergence
- Under ideal conditions (optimal discriminator) this is equivalent to choosing for d(p_data, p_model) JSD instead of KLD


Problems
- Unstable optimization
	- Do not know when to stop, as with MLE when you can see that loss no longer improves
	- Why?
- Mode collapse
	- Focus on a few types of data points (in training set?) and completely stop generating others
	- few types of data points - they called modes
	- How to protect generator from memorizing the training set?
		- It does not have the access to it!
- Evaluation


GAN
- likelihood-free training

## f-GAN

f-Divergences
- Broader class of divergences

Fenchel conjugate
- https://chatgpt.com/c/675da715-8e20-8000-bdec-ea26fcaae9e8
- convex conjugate
- best linear approximation
- duality
- [[Convex Optimization]]

supremum
- https://en.wikipedia.org/wiki/Infimum_and_supremum
- supremum = least upper bound

f-GAN
- minimizing lower bound of f-Divergence

Why it's better than DKL?
- DKL == Compression
- Might not be what we want

## Wasserstein GAN

Support of a distribution: set of points that have non-zero probability

Problems with f-Divergence
- Arises when p and q have disjoint supports
- No signal from training in this case since Divergence always equal to some constant

Wasserstein distance
- Earth-Mover distance

Lipschitz constant
- TODO
- Lipschitz constant of f(x) is 1


## Inferring latent representation

Solution 1
- Use features from discriminator

Solution 2
- How to use generator?
- BiGAN
	- Encoder is deterministic mapping unlike in VAE


# Lecture 11, 12 - Energy Based Models

[[Energy-Based Models (EBMs)]]

DM
- EBM is closely related to DM
- As well as NF and VAE

EBM
- allows flexible model architectures
- stable training
- likelihood based
- inspired by statistical physics

Probability distribution p(x)
- Parameterizing probability distribution p(x)
- Not an arbitrary functions
- 2 restrictions
	- non-negative
		- Easy to satisfy
	- sum to 1
		- Trickier to satisfy
		- Intuition why we need
			- Total volume is fixed, increasing one p(x_1) we are sure that probability will decrease for all other xs
		- How this property achieved in other models?
			- NF - clear
			- VAE - ?
			- Autoregressive - ?
			- TODO
- Partition function - the integral of all space, all the volume
- Choose our function to be able to compute this partition function analytically
	- Example: [[Normal Distribution]]
	- Exponential family
	- [[Ветров. Введение в вероятностный язык построения моделей машинного обучения]]

Likelihood-based learning
- Autoregressive
- Latent Variables
- Why it's normalized by construction?
- Why VAE not in this list?
	- TODO

EBM
- Why exponent? Not x^2 ?
- vs. Softmax
	- Partition function can be computed exactly and easily
	- Learning just p(y|x)
- In general case the partition function is very hard to compute, if we model p(x)

Application of EBM
- When some task do not requires partition function
- Compare two data samples
	- Classification
	- Anomaly Detection
	- Denoising
- Ising Model
	- https://chatgpt.com/c/67654fca-9eb4-8000-bddd-94f379a58b3d

Product of Experts
- TODO

Restricted Boltzmann machine (RBM)
- TODO

## Training

Training intuition
- Just increasing unnormalized log-probability do not guarantee that this example becomes more likely
- Increase probability of train datapoint and decrease probability of everything else

Contrastive Divergence Algorithm
- Monte-Carlo estimate for partition function using negative samples
- Negative samples - samples from the model


## Sampling

How to get samples?

We need samples during training as well, not only during inference

Markov Chain Monte Carlo (MCMC)
- Use the fact we can do comparison
- Metropolis-Hastings Markov chain Monte Carlo
	- ?
- accept
- reject - we can also accept in this case with some probability which depends on how bad the new version of the image are
- Works in theory, in practice need a large number of steps to converge
- Convergence slows down exponentially with increasing dimensions

Unadjusted Langevin MCMC
- Special case of above
- Noisy gradient ascent
- You do not need accept or reject
- Unadjusted
	- TODO
- Convergence is still slow
- Very expensive training since this sampling is required on each step

Training without sampling
- Score matching
- Noise Contrastive Estimation
- Adversarial training

## Score matching

Score function
- Gradient of log probability with respect to x, not theta
- Function of x, and theta?! (see example with gaussian where theta is just two numbers - mean and variance)
	- parameterized by theta
- Why gradient of log?
	- log is monotonic, so gradient points to the same direction(and with the same magnitude) as for original PDF?
	- math is easy for EBM
- Alternative interpretation of PDF as a vector field, instead of scalar field
- Equivalent in Physics: electric field and potential
- Computationally it might my more handy to use one vs. another
	- Potential is more handy in Physics!
		- A
	- Field is more handy in DL

Score matching
- Fisher divergence
	- Difference between gradients of two distributions
	- If two distribution p and q are similar, they also should have similar gradients
- We do not know the gradient of the true data distribution
- Integration by parts
- Hessian is expensive
- Intuition of the loss function
	- Finding flat regions where gradient vectors are small
	- Hessian to ensure it's maximum, not minimum
- Q
	- When learning will we compute the gradient for gradient?
		- First - gradient wrt x
		- Second - gradient wrt theta, for updating params

Recap
- KL Divergence == MLE
	- Contrastive divergence
	- need to sample from model for training
- Fisher Divergence == Score matching
	- no need to sample


## NCE

Noise Contrastive Estimation (NCE)
- Noise distribution is fixed
- Training discriminator
- Optimal discriminator which use EBM inside
- Partition function as learnable parameter
	- Just a scaler?
	- Why we didn't do this earlier?

NCE vs. GAN

Flow Contrastive Estimation
- Use NF for generating noise as close to the true data distribution as we can to make work of discriminator harder
- Looks a lot like a GAN


# Lecture 13 - Score Based Models

Is score matching limited to EBMs?
- NO

Model the score function
- Instead of modeling the energy, we are directly model the score function.
- Broader family of models.
	- The gradient you are modeling might be not ...
		- the gradient of a scalar function
		- conservative vector field
	- There might not be an underlying scalar function
- Output of NN now is a gradient vector, not scalar?
- R^d -> R^d 


How to compare two vector fields?
- Fisher divergence?!
	- Average Euclidian distance over the all space


Jacobian -> Hessian
- https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant
- https://en.wikipedia.org/wiki/Hessian_matrix
- [[Hessian]]


Score matching is not scalable
- Trace of the Jacobian is the hard part
- O(d) backprop, where d is the number of dimension
	- TODO
	- We need to do several different backprop passes?

Denoising score matching
- Vincent, 2011
- Estimate score of the noised version of the distribution
	- noise-perturbed data distribution
	- q(x') - very complex
- Perturbation/distribution kernel
	- q(x'|x) - simple Gaussian
- Stein unbiased risk estimator
- Score matching reduces to denoising
- Loss formula
	- Why plus?
- Pros
	- Scalable
- Cons
	- Estimates scores of noise-perturbed data
		- How it affect the quality of samples?
		- We can generate only noisy samples! More about this later...
- Q:
	- Is the gradient of the disturbed image should be similar to the noise?
		- Yes
		- But why, if we estimates the gradient of the noise-perturbed data?
		- Tweedie's formula
	- Why we can't use the same model if we train it to predict the noise?
		- [chatgpt](https://chatgpt.com/g/g-p-676b182bc0e8819196b1f598b592448e-generative-models/c/676b1a2b-3ea8-8000-80ea-a24a636f3d09)
		- Получается что даже умея предсказывать тот шум который был добавлен мы не можем получить сэмпл из исходного распределения используя Langevin dynamic. 
		- Что если просто вычитать его из изображения?
			- Это тоже самое что и следовать градиенту
		- Ответ в том что в любом случае мы знаем только направление? Мы не учитываем длину вектора шума, когда обучаем модель?

Sliced score matching
- Comparing projections of the vectors instead of comparing them fully
- Random projections
- Pros
	- Estimates true data density

Sampling
- Langevin MCMC

Pitfalls
- Data lies in manifolds
	- Score function might be not well defined
	- TODO
- Challenges in low data density regions
- Mixing of Langevin dynamic
	- Example with mixture of two distributions
	- Modes of data distribution


# Lecture 14

Connection between score-based models and [[Diffusion Models]]

The solution to all above pitfalls: Gaussian perturbation

How much noise to add?
- Tradeoff
	- Add as little noise as possible to estimate something close to the real data
	- The more noise you add the better estimation becomes

Solution
- Using multiple noise scales
- multiple noise scales -> Annealed Langevin dynamic
- Noise conditional score networks (NCSN)

Sliced score matching for noise-perturbed data?
- TODO

Loss
- jointly solve many problems for different noise levels
- weighting factor

Choosing noise scales
- max - maximum pairwise distance between datapoints
- min - samples should looks like clean images

Infinite number of noise levels
- See picture from the slide!
- What represent the path of the image on this picture?
	- Sequence of random variables each drawn from their own distribution
	- Stochastic process
	- SDE
- Generation via reversing stochastic process
	- Similar on solving SDE by numerical methods
	- Euler-Maruyama
	- Same as Langevin dynamic
	- Now we can use a lot of advanced numerical methods for solving SDE for our task of image generation
- SDE
	- Forward SDE
	- Reverse SDE
		- Score function
		- What this score function mean? Previously we had many score function for each noise level. Same thing here?
- Training
	- Estimate the score function as we did before, but now t can take infinite number of values

Converting SDE to ODE
ODE and NF

# LAST