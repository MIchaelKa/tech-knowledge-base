
Deep Generative Models
Stefano Ermon

# External

- https://deepgenerativemodels.github.io/
- https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8
	- 2023

CS236 homeworks
- https://github.com/Subhajit135/CS236_DGM
- https://deepgenerativemodels.github.io/assignments.html

# Links

[[Generative Models]]

[[Bayesian Statistics]]

[[Probability Theory]]

# Notes


# Lecture 1 - Introduction

- Computer Vision as inverse task of image generation
- Statistical generative models
	- Model is a probability distribution
	- Dataset - samples from that distribution
	- After we train the model we can sample from that distribution
	- Have we access to the scalar probability value?
- Examples of using
	- Outlier detection
	- Super-resolution
	- Text2Speech
	- Machine translation as generative problem
- Control signal
	- text
	- low resolution
	- greyscale image
- Inverse problems
	- ?
- Challenges
	- Representation
		- How to model the joint dist. of many r.v. (ex.: different pixels if an image) ? 
	- Learning
		- How to compare probability distributions?


# Lecture 2 - Background

- Model family
	- Set of all possible distributions 
- Two questions
	- How to represent probability distribution?
	- How to learn them?
		- Need to define some notion of distance between two distributions
		- Loss
- Challenges
	- [[Curse of Dimensionality]]
- Bernoulli distribution
	- Only one parameter
	- We can learn it and generate new data
	- Is it possible to pose classification problem?
- Categorical
- Example of joint distribution
	- Modeling a color of a single pixel value
	- RGB channels
	- How many parameters we need?
		- $255^3 -1$ - if all channels values depends on each other
		- $255*3$ - if we can assume that channels are independent
- Example of joint distribution
	- black and white images
	- state space is $2^n$, where n is the number of pixels
	- sampling from $p(x_1, ... x_n)$ will generate an image
- Assumptions to deal with large state space
	- Links
		- [[Sum Rule]]
	- Independence
		- Join distribution can be factored as a product of marginal distributions
		- Parameters: n
- Two important rules
	- Chain Rule
	- Bayes' Rule
		- [[Bayes’ Theorem]]
		- Why it's important for application together with chain rule?
-  Chain Rule
	- Autoregressive models
	- Language modeling
	- [[Joint Probability]]
	- Parameters: $1 + 2 + ... + 2^{n-1}$
	- We still did not make any assumptions
	- Conditional independence
		- Markov chain assumption
		- Parameters: $2n - 1$
		- Big savings, much more reasonable model
		- BOW
- Valid PD
	- TODO
- Bayes Network
	- Conditionals a little bit more complex than when we just use chain rule
	- Condition on a set o variable
	- Bayesian Network
	- Not related to Neural Networks
	- Tabular representation
- Bayesian Network
	- Data structure to specify a probability distribution
	- DAG
	- Number of nodes equal to number of rv
	- You can change expressiveness of the network between markovian (1 parent for node) and fully defined joint distribution
	- vs. [[Probabilistic Graphical Model (PGM)]]
	- implies conditional independence
- generative vs. discriminative
	- discriminative
		- What we want to model  is still complex distribution $p(Y, X_1, ... X_n)$ with $2^{n-1}$ parameters
	- generative
		- you can predict anything from anything
		- impute missing things
		- more complex task
		- you need to model distribution between features as well
- Naive Bayes
	- generative model for discriminative task
	- [[Naive Bayes]]
	- Naive Bayes for single label prediction
	- All models are wrong, but some of them are usefull
- Logistic regression
	- intractable to have the full table with 2^n rows (parameters) for each possible combination of inputs, parameters in this table is probability of the label Y (100% or 0% ?)
	- assuming that there is some simple function
		- linear dependence assumption
	- Q: Is Logistic regression implies some independence assumption?
		- No
	- vs. Naive Bayes 
		- assuming less than Naive Bayes about how your variables are related, good if you have a lot of data
		- if you have less data we can try to use Naive Bayes
			- prior helps you more
- Neural Network
	- non-linear dependence
- Continuous variables
	- [[Normal Distribution]]
	- Mixture of two Gaussians
		- one rv is Bernulli
	

# Lecture 3 - Autoregressive Models

- Density estimation
	- p(x)
	- useful by itself
	- provides you a way to train a model using MLE
- How to represent probability distribution?
	- $p(x_1, ... x_{728})$
	- Chain Rule == Autoregressive Models
- Autoregressive Models
	- Ordering
		- Not clear which one to use for images
		- Pick an ordering of all rv
		- Example
			- raster scan ordering
	- Context
		- Length of context
	- RNN
	- Transformer
- FVSBN
	- Fully Visible Sigmoid Belief Networks (FVSBN)
	- FVBN
	- chain rule
	- LR
		- n-1 logistic regression problems
		- more and more features for LR
	- How to evaluate?
		- Product of LR functions
	- How to sample?
		- sample first pixel
		- use chain rule
	- Pros
		- simple technique
	- Cons
		- sequential 
	- How many parameters?
		- $1+2+3+...+n = n^2 / 2$
- NADE
	- Neural Autoregressive Density Estimation
	- Tie weights
		- reduce the number of parameters
		- speed up computation
			- how?
			- O(nd)
		- what assumption we make tying weights?
- Common
	- Conditional generation
	- Conditional generation vs. Modeling joint distribution
	- Label is just one more feature for generative models, but this feature can have much bigger influence
- RNADE
	- Mixture of K gaussians
	- vs. just one parameter as we have in linear regression ???
	- https://chatgpt.com/c/6751fca9-e6fc-8000-8c00-51c708f0c9c3
- Autoregressive models vs. Autoencoders
	- Can we get a generative model from AE?
	- Enforce some ordering
- MADE
	- Germain et al., 2015
	- Masked Autoencoder for Distribution Estimation
	- Ordering can be anything
	- Only training time benefit
	- Inference is still sequential
- RNN
	- [[RNN]]
	- No Markov assumption
	- [[Markov Chain]]
	- Cons
		- Sequential evaluation, cannot be parallelized
			- TODO
			- Cannot pack the sequence in one batch as transformer can.
			- Can we still can compose a batch from different samples?
- PixelRNN
	- Masking as in MADE
		- TODO
- PixelCNN
- Adversarial attacks and Anomaly detection
	- PixelDefend
		- Song et al., 2018
		- How to get p(x) from PixelCNN?
- Autoregressive Models
	- Pros
		- Easy to evaluate likelihood
			- Density estimation
			- How to get p(x) from PixelCNN?
	- Cons
		- No natural way to get features
- Links
	- How to get p(x) from PixelCNN?
		- https://chatgpt.com/c/674f0fca-7614-8000-902c-740781a42cbc

# Lecture 4 - Maximum Likelihood Learning

Learning a distribution
- we have samples only for most probable points in the space and don't have any data for less probable
- we want the best approximation of our data distribution, but what is the best mean?
- joint distribution of images and text
	- CLIP?
	- how we can imagine text distribution, where each text sequence has different length?
		- pad
		- chain rule

How should we measure distance between distributions?
- Autoregressive models
	- We can use likelihood since we have access to it
	- KL-Divergence
		- we care about compression
			- it may be not what you want
		- two options
			- E_{x~p_{data}}
				- only dataset is matter
			- E_{x~p_{model}}
				- reverse KL
				- I want the low loss on the things I generate myself
				- But how to compare them with p_data, if we don't know it outside of the dataset?
			- precision vs. recall
				- recall - cover all the dataset
				- how good is that we cover
		- minimizing KL-Div == maximizing expected log-likelihood
			- approximate expected log-likelihood with empirical log-likelihood
		- cons
			- we don't know how close we are to the optimum
				- we don't know the constant term
			- мы можем сравнить две модели, но не знаем насколько хороши мы в принципе

Training
- Regularization as reducing model family set
- If you have two model with similar training loss, prefer smaller one


# Lecture 5,6 - VAEs

11:45

Latent variable models

[[Variational Autoencoder (VAE)]]

Motivation
- Features: gender, hair color, etc., as factors variability
	- We do not have access to it (unless it explicitly annotated)
	- It can be useful for building a distribution.
	- Separate models for conditioning on these features
	- Using latent z variable is like clustering (by color, gender, etc.)
- If we had trained this model, then we could identify features via p(z | x)
	- Should we able to compute p(x | z) for it?
	- https://chatgpt.com/c/6752c69f-4660-8000-9ba9-3e9abfde395f

Mixture of Gaussians
- Shallow latent variable model
- Marginal can be very flexible/complex despite it consist of simple gaussians

VAE
- Mixture of infinite gaussians, z~N(0,I)
- Can finite number of gaussians works better?
	- VQ?
- Even though p(x | z) is simple, the marginal p(x) is very complex

Evaluating likelihood
- First attempt: Naive Monte-Carlo
	- Sampling Z from uniform distribution is a bad solution
	- What stops us from sampling z from  ... ?
- Second attempt: Importance Sampling
	- gpt
		- https://chatgpt.com/c/67531880-bcb0-8000-a3f2-644d609365d9
		- https://chatgpt.com/c/6753381b-e784-8000-86ef-8962994dc0e4
	- If we directly take the logarithm of the Monte Carlo estimate the result becomes biased.
	- expectation of the log - true training objection
		- this is not right ???
	- The Monte Carlo estimate is unbiased, but its log is not.
	- [[Jensen’s Inequality]]
	- $log(E[f(x,z)])$ vs. $E[log(f(x,z))]$
	- only one sample!
		- because we have only one sample we end up with expectation of the log


Unsupervised Learning
- Clustering and latent z
- extension of k-means
- We can use latent z for clustering objects
- Example: all people with the same eyes color in one cluster

Laten Z
- Can we have more complex distribution for it, instead of using standart normal?
- Learn standart distribution parameters
- Predict z with another VAE
	- Hierarchical / StackedVAEs
	- Diffusion models - DM

How to build more powerful models
- More flexible models
- Stacking VAEs vs. Adding layers vs. Adding dimensions to z


Variational Inference
- E step in [[EM Algorithm]]
- Learning model when you have missing data

Loglikelihood as if fully observed
- https://chatgpt.com/c/6754873a-9b3c-8000-b6b4-b213a4c3adb3

Intractable posterior
- Can we use a mixture of gaussians for approximating p(z | x), instead of modeling it by q which is parameterized to define just one gaussian
	- Answering here https://youtu.be/8cO61e_8oPY?si=NiXqVcsxU_wbks2r&t=2230
	- Using autoregressive model?

Optimization
- Optimizing family of lower bounds
- Family because of q distribution, different q gives us different lower bound
- M and E steps in [[EM Algorithm]]

Inference
- Density estimation - use q
- Generation - use p

ELBO for entire dataset
- Different q parameters for each datapoint
- Example when we modeling top half of the image by having independent parameter for each pixel
- But what if we have neural net which can infer needed top half given bottom?
	- This is what is called Amortized Inference

Optimization 2
- step on q params
- step on p params
- joint optimization?

Gradients for p params
- Slide 13
- How we get L2 or cross-entropy loss from optimizing log(p(x,z)) ?
	- TODO
	- https://youtu.be/m6dKKRsZwBQ?si=nW_GmZ-Pw1j3QqII&t=1118
- Exactly same gradient as in autoregressive models...
- Maximum probability of the train image at the mean of gaussian distribution where this mean is equal to this image.

Gradients for q params
- Reinforce
	- [[Reinforcement Learning (RL)]]
	- Used in VQ?
- Reparameterization trick
	- less general
	- only works when latent z is continuous
	- rv does not depend on q params anymore
- How to push gradient through Expectation
	- when gradient and expectation depends on the same variable
	- grad E -> E grad

VAE
- AE + defining prior distribution for z = generative model = VAE


# Lecture 7,8 - Normalizing Flows


Links
- Normalizing Flows Tutorial
- https://blog.evjang.com/2018/01/nf1.html

Points
- Another way to going around intractable marginal probability in a latent variable model.
- How to design a latent variable model with tractable likelihood?
- In VAE it's hard to get p(x)?
	- Using q(z|x)
- You can think of diffusion models as certain kind of flow models
	- infinitely deep?
	- diffusion models (DM) unify flow models and VAE

Flow models
- Key idea: Map simple distribution to complex using invertible transformations
- What if we can easily invert p(x|z) and construct p(z|x)
	- Make x = f(z) deterministic invertible function
	- For any x we have unique z
- Flow models is VAE when mapping from z to x is deterministic and invertible
- Now x and z have the same number of dimensions
	- Why we have such restriction?
	- If it's not, the mapping would not be invertible.
		- Why?
		- For every x will be several corresponding z
- Generation process is deterministic
	- Limitation?
	- How to get different samples from one z in VAE?
		- z is computed using unit variance and zero mean

Background
- Change of variables formula
- Random vectors
	- Linear case: matrix and determinant
	- Non-linear case: determinant of Jacobian
		- How volume is stretched locally

Normalizing Flow
- Flow of transformations
- Note: determinant of product is equal to product of determinants
- Stack of neural layers
	- Should we handle non-linear layers somehow?

Flow models intuition
- It's hard to model some complicated dataset using simple n-D gaussian
- It becomes possible after applying several transformations to the original density function
- Find PMF that put more probability mass to the datapoints that you have in your training set

Learning and inference
- Learning
	- x -> z
	- Exact likelihood evaluation using inverse transformation
- Inference
	- z -> x
	- Forward transformation

Jacobian matrix
- n x n,  where n is the data dimensionality
- pretty large matrix, need to compute determinant efficiently
- Choose transformation so that Jacobian has special structure
	- For example triangular matrix -> we can find determinant in O(n)

Triangular Jacobian
- f_i(z) depends only on z <= i
- similar to autoregressive models
	- masked AE?

NICE
- Nonlinear Independent Components Estimation
- (Dinh et al., 2014)
- Volume preserving transformation (determinant is 1)
- only shift

Real-NVP
- Non-volume preserving extension of NICE
- shift + scale

Autoregressive models as Flow models
- Continuos Gaussian autoregressive models


MAF
- Masked Autoregressive Flow
- Connection with MADE
- training is efficient
- generation is sequential and slow

IAF
- Inverse Autoregressive Flow
- generation is parallel and efficient
- training is slow
	- but can efficiently calculate likelihood for generated samples, this property is used in Parallel Wavenet

Parallel Wavenet
- Paper
	- Parallel WaveNet: Fast High-Fidelity Speech Synthesis
	- https://arxiv.org/abs/1711.10433
	- Google
- Speech Synthesis
- MAF + IAF + Teacher-Student Distillation
- Probability Density Distillation
- D_KL(s, t)
	- important to sample from s model for monte-carlo estimation of above this D_KL
- Q:
	- Can we do the same trick for language model?
	- No, because X is discrete?
		- TODO
	- But ok for images?
		- 0 - 255 is also is discrete space but we can add noise, and previously it was mentioned that this is not the problem for images
		- See: Reparameterization trick
	- Do we have the requirement of continuous data for Flow model?
		- Yes, only applicable to PDF not PMF


MintNet
- Is it possible to define invertible layers that is convolutional?
- Masked convolutions
- vs. PixelCNN


Gaussianization Flow
- Inverse CDF Trick
- [[Histogram Equalization]]

# LAST