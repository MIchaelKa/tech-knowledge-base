
Deep Generative Models
Stefano Ermon

# External

- https://deepgenerativemodels.github.io/
- https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8
	- 2023

CS236 homeworks
- https://github.com/Subhajit135/CS236_DGM
- https://deepgenerativemodels.github.io/assignments.html

# Links

[[Generative Models]]

[[Bayesian Statistics]]

[[Probability Theory]]

# Notes


# Lecture 1 - Introduction

- Computer Vision as inverse task of image generation
- Statistical generative models
	- Model is a probability distribution
	- Dataset - samples from that distribution
	- After we train the model we can sample from that distribution
	- Have we access to the scalar probability value?
- Examples of using
	- Outlier detection
	- Super-resolution
	- Text2Speech
	- Machine translation as generative problem
- Control signal
	- text
	- low resolution
	- greyscale image
- Inverse problems
	- ?
- Challenges
	- Representation
		- How to model the joint dist. of many r.v. (ex.: different pixels if an image) ? 
	- Learning
		- How to compare probability distributions?


# Lecture 2 - Background

- Model family
	- Set of all possible distributions 
- Two questions
	- How to represent probability distribution?
	- How to learn them?
		- Need to define some notion of distance between two distributions
		- Loss
- Challenges
	- [[Curse of Dimensionality]]
- Bernoulli distribution
	- Only one parameter
	- We can learn it and generate new data
	- Is it possible to pose classification problem?
- Categorical
- Example of joint distribution
	- Modeling a color of a single pixel value
	- RGB channels
	- How many parameters we need?
		- $255^3 -1$ - if all channels values depends on each other
		- $255*3$ - if we can assume that channels are independent
- Example of joint distribution
	- black and white images
	- state space is $2^n$, where n is the number of pixels
	- sampling from $p(x_1, ... x_n)$ will generate an image
- Assumptions to deal with large state space
	- Links
		- [[Sum Rule]]
	- Independence
		- Join distribution can be factored as a product of marginal distributions
		- Parameters: n
- Two important rules
	- Chain Rule
	- Bayes' Rule
		- [[Bayes’ Theorem]]
		- Why it's important for application together with chain rule?
-  Chain Rule
	- Autoregressive models
	- Language modeling
	- [[Joint Probability]]
	- Parameters: $1 + 2 + ... + 2^{n-1}$
	- We still did not make any assumptions
	- Conditional independence
		- Markov chain assumption
		- Parameters: $2n - 1$
		- Big savings, much more reasonable model
		- BOW
- Valid PD
	- TODO
- Bayes Network
	- Conditionals a little bit more complex than when we just use chain rule
	- Condition on a set o variable
	- Bayesian Network
	- Not related to Neural Networks
	- Tabular representation
- Bayesian Network
	- Data structure to specify a probability distribution
	- DAG
	- Number of nodes equal to number of rv
	- You can change expressiveness of the network between markovian (1 parent for node) and fully defined joint distribution
	- vs. [[Probabilistic Graphical Model (PGM)]]
	- implies conditional independence
- generative vs. discriminative
	- discriminative
		- What we want to model  is still complex distribution $p(Y, X_1, ... X_n)$ with $2^{n-1}$ parameters
	- generative
		- you can predict anything from anything
		- impute missing things
		- more complex task
		- you need to model distribution between features as well
- Naive Bayes
	- generative model for discriminative task
	- [[Naive Bayes]]
	- Naive Bayes for single label prediction
	- All models are wrong, but some of them are usefull
- Logistic regression
	- intractable to have the full table with 2^n rows (parameters) for each possible combination of inputs, parameters in this table is probability of the label Y (100% or 0% ?)
	- assuming that there is some simple function
		- linear dependence assumption
	- Q: Is Logistic regression implies some independence assumption?
		- No
	- vs. Naive Bayes 
		- assuming less than Naive Bayes about how your variables are related, good if you have a lot of data
		- if you have less data we can try to use Naive Bayes
			- prior helps you more
- Neural Network
	- non-linear dependence
- Continuous variables
	- [[Normal Distribution]]
	- Mixture of two Gaussians
		- one rv is Bernulli
	

# Lecture 3 - Autoregressive Models

- Density estimation
	- p(x)
	- useful by itself
	- provides you a way to train a model using MLE
- How to represent probability distribution?
	- $p(x_1, ... x_{728})$
	- Chain Rule == Autoregressive Models
- Autoregressive Models
	- Ordering
		- Not clear which one to use for images
		- Pick an ordering of all rv
		- Example
			- raster scan ordering
	- Context
		- Length of context
	- RNN
	- Transformer
- FVSBN
	- Fully Visible Sigmoid Belief Networks (FVSBN)
	- FVBN
	- chain rule
	- LR
		- n-1 logistic regression problems
		- more and more features for LR
	- How to evaluate?
		- Product of LR functions
	- How to sample?
		- sample first pixel
		- use chain rule
	- Pros
		- simple technique
	- Cons
		- sequential 
	- How many parameters?
		- $1+2+3+...+n = n^2 / 2$
- NADE
	- Neural Autoregressive Density Estimation
	- Tie weights
		- reduce the number of parameters
		- speed up computation
			- how?
			- O(nd)
		- what assumption we make tying weights?
- Common
	- Conditional generation
	- Conditional generation vs. Modeling joint distribution
	- Label is just one more feature for generative models, but this feature can have much bigger influence
- RNADE
	- Mixture of K gaussians
	- vs. just one parameter as we have in linear regression ???
	- https://chatgpt.com/c/6751fca9-e6fc-8000-8c00-51c708f0c9c3
- Autoregressive models vs. Autoencoders
	- Can we get a generative model from AE?
	- Enforce some ordering
- MADE
	- Germain et al., 2015
	- Masked Autoencoder for Distribution Estimation
	- Ordering can be anything
	- Only training time benefit
	- Inference is still sequential
- RNN
	- [[RNN]]
	- No Markov assumption
	- [[Markov Chain]]
	- Cons
		- Sequential evaluation, cannot be parallelized
			- TODO
			- Cannot pack the sequence in one batch as transformer can.
			- Can we still can compose a batch from different samples?
- PixelRNN
	- Masking as in MADE
		- TODO
- PixelCNN
- Adversarial attacks and Anomaly detection
	- PixelDefend
		- Song et al., 2018
		- How to get p(x) from PixelCNN?
- Autoregressive Models
	- Pros
		- Easy to evaluate likelihood
			- Density estimation
			- How to get p(x) from PixelCNN?
	- Cons
		- No natural way to get features
- Links
	- How to get p(x) from PixelCNN?
		- https://chatgpt.com/c/674f0fca-7614-8000-902c-740781a42cbc

# Lecture 4 - MLE

Maximum Likelihood Learning

Learning a distribution
- we have samples only for most probable points in the space and don't have any data for less probable
- we want the best approximation of our data distribution, but what is the best mean?
- joint distribution of images and text
	- CLIP?
	- how we can imagine text distribution, where each text sequence has different length?
		- pad
		- chain rule

How should we measure distance between distributions?
- Autoregressive models
	- We can use likelihood since we have access to it
	- KL-Divergence
		- we care about compression
			- it may be not what you want
		- two options
			- E_{x~p_{data}}
				- only dataset is matter
			- E_{x~p_{model}}
				- reverse KL
				- I want the low loss on the things I generate myself
				- But how to compare them with p_data, if we don't know it outside of the dataset?
			- precision vs. recall
				- recall - cover all the dataset
				- how good is that we cover
		- minimizing KL-Div == maximizing expected log-likelihood
			- approximate expected log-likelihood with empirical log-likelihood
		- cons
			- we don't know how close we are to the optimum
				- we don't know the constant term
			- мы можем сравнить две модели, но не знаем насколько хороши мы в принципе

Training
- Regularization as reducing model family set
- If you have two model with similar training loss, prefer smaller one


# Lecture 5,6 - VAEs

11:45

Latent variable models

[[Variational Autoencoder (VAE)]]

Motivation
- Features: gender, hair color, etc., as factors variability
	- We do not have access to it (unless it explicitly annotated)
	- It can be useful for building a distribution.
	- Separate models for conditioning on these features
	- Using latent z variable is like clustering (by color, gender, etc.)
- If we had trained this model, then we could identify features via p(z | x)
	- Should we able to compute p(x | z) for it?
	- https://chatgpt.com/c/6752c69f-4660-8000-9ba9-3e9abfde395f

Mixture of Gaussians
- Shallow latent variable model
- Marginal can be very flexible/complex despite it consist of simple gaussians

VAE
- Mixture of infinite gaussians, z~N(0,I)
- Can finite number of gaussians works better?
	- VQ?
- Even though p(x | z) is simple, the marginal p(x) is very complex

Evaluating likelihood
- First attempt: Naive Monte-Carlo
	- Sampling Z from uniform distribution is a bad solution
	- What stops us from sampling z from  ... ?
- Second attempt: Importance Sampling
	- gpt
		- https://chatgpt.com/c/67531880-bcb0-8000-a3f2-644d609365d9
		- https://chatgpt.com/c/6753381b-e784-8000-86ef-8962994dc0e4
	- If we directly take the logarithm of the Monte Carlo estimate the result becomes biased.
	- expectation of the log - true training objection
		- this is not right ???
	- The Monte Carlo estimate is unbiased, but its log is not.
	- [[Jensen’s Inequality]]
	- $log(E[f(x,z)])$ vs. $E[log(f(x,z))]$
	- only one sample!
		- because we have only one sample we end up with expectation of the log


Unsupervised Learning
- Clustering and latent z
- extension of k-means
- We can use latent z for clustering objects
- Example: all people with the same eyes color in one cluster

Laten Z
- Can we have more complex distribution for it, instead of using standart normal?
- Learn standart distribution parameters
- Predict z with another VAE
	- Hierarchical / StackedVAEs
	- Diffusion models - DM

How to build more powerful models
- More flexible models
- Stacking VAEs vs. Adding layers vs. Adding dimensions to z


Variational Inference
- E step in [[EM Algorithm]]
- Learning model when you have missing data

Loglikelihood as if fully observed
- https://chatgpt.com/c/6754873a-9b3c-8000-b6b4-b213a4c3adb3

Intractable posterior
- Can we use a mixture of gaussians for approximating p(z | x), instead of modeling it by q which is parameterized to define just one gaussian
	- Answering here https://youtu.be/8cO61e_8oPY?si=NiXqVcsxU_wbks2r&t=2230
	- Using autoregressive model?

Optimization
- Optimizing family of lower bounds
- Family because of q distribution, different q gives us different lower bound
- M and E steps in [[EM Algorithm]]

Inference
- Density estimation - use q
- Generation - use p

ELBO for entire dataset
- Different q parameters for each datapoint
- Example when we modeling top half of the image by having independent parameter for each pixel
- But what if we have neural net which can infer needed top half given bottom?
	- This is what is called Amortized Inference

Optimization 2
- step on q params
- step on p params
- joint optimization?

Gradients for p params
- Slide 13
- How we get L2 or cross-entropy loss from optimizing log(p(x,z)) ?
	- TODO
	- https://youtu.be/m6dKKRsZwBQ?si=nW_GmZ-Pw1j3QqII&t=1118
- Exactly same gradient as in autoregressive models...
- Maximum probability of the train image at the mean of gaussian distribution where this mean is equal to this image.

Gradients for q params
- Reinforce
	- [[Reinforcement Learning (RL)]]
	- Used in VQ?
- Reparameterization trick
	- less general
	- only works when latent z is continuous
	- rv does not depend on q params anymore
- How to push gradient through Expectation
	- when gradient and expectation depends on the same variable
	- grad E -> E grad

VAE
- AE + defining prior distribution for z = generative model = VAE


# Lecture 7,8 - Normalizing Flows


Links
- Normalizing Flows Tutorial
- https://blog.evjang.com/2018/01/nf1.html

Points
- Another way to going around intractable marginal probability in a latent variable model.
- How to design a latent variable model with tractable likelihood?
- In VAE it's hard to get p(x)?
	- Using q(z|x)
- You can think of diffusion models as certain kind of flow models
	- infinitely deep?
	- diffusion models (DM) unify flow models and VAE

Flow models
- Key idea: Map simple distribution to complex using invertible transformations
- What if we can easily invert p(x|z) and construct p(z|x)
	- Make x = f(z) deterministic invertible function
	- For any x we have unique z
- Flow models is VAE when mapping from z to x is deterministic and invertible
- Now x and z have the same number of dimensions
	- Why we have such restriction?
	- If it's not, the mapping would not be invertible.
		- Why?
		- For every x will be several corresponding z
- Generation process is deterministic
	- Limitation?
	- How to get different samples from one z in VAE?
		- z is computed using unit variance and zero mean

Background
- Change of variables formula
- Random vectors
	- Linear case: matrix and determinant
	- Non-linear case: determinant of Jacobian
		- How volume is stretched locally

Normalizing Flow
- Flow of transformations
- Note: determinant of product is equal to product of determinants
- Stack of neural layers
	- Should we handle non-linear layers somehow?

Flow models intuition
- It's hard to model some complicated dataset using simple n-D gaussian
- It becomes possible after applying several transformations to the original density function
- Find PMF that put more probability mass to the datapoints that you have in your training set

Learning and inference
- Learning
	- x -> z
	- Exact likelihood evaluation using inverse transformation
- Inference
	- z -> x
	- Forward transformation

Jacobian matrix
- n x n,  where n is the data dimensionality
- pretty large matrix, need to compute determinant efficiently
- Choose transformation so that Jacobian has special structure
	- For example triangular matrix -> we can find determinant in O(n)

Triangular Jacobian
- f_i(z) depends only on z <= i
- similar to autoregressive models
	- masked AE?

NICE
- Nonlinear Independent Components Estimation
- (Dinh et al., 2014)
- Volume preserving transformation (determinant is 1)
- only shift

Real-NVP
- Non-volume preserving extension of NICE
- shift + scale

Autoregressive models as Flow models
- Continuos Gaussian autoregressive models


MAF
- Masked Autoregressive Flow
- Connection with MADE
- training is efficient
- generation is sequential and slow

IAF
- Inverse Autoregressive Flow
- generation is parallel and efficient
- training is slow
	- but can efficiently calculate likelihood for generated samples, this property is used in Parallel Wavenet

Parallel Wavenet
- Paper
	- Parallel WaveNet: Fast High-Fidelity Speech Synthesis
	- https://arxiv.org/abs/1711.10433
	- Google
- Speech Synthesis
- MAF + IAF + Teacher-Student Distillation
- Probability Density Distillation
- D_KL(s, t)
	- important to sample from s model for monte-carlo estimation of above this D_KL
- Q:
	- Can we do the same trick for language model?
	- No, because X is discrete?
		- TODO
	- But ok for images?
		- 0 - 255 is also is discrete space but we can add noise, and previously it was mentioned that this is not the problem for images
		- See: Reparameterization trick
	- Do we have the requirement of continuous data for Flow model?
		- Yes, only applicable to PDF not PMF


MintNet
- Is it possible to define invertible layers that is convolutional?
- Masked convolutions
- vs. PixelCNN


Gaussianization Flow
- Inverse CDF Trick
- [[Histogram Equalization]]
- TODO

# Lecture 9,10 - GANs

[[Generative Adversarial Networks (GAN)]]

MLE
- All previous models
- log-likelihood
- D_KL ==> MLE
	- Show this
	- TODO
- We don't need to know the true data distribution to calculate D_KL(P_data || P_model) ?
	- We use data samples to estimate the expectation
	- We can only optimize D_KL up to the shift!
- How to compare probability distributions in a different way?
- Why MLE?
	- Optimal statistical efficiency
	- Compression and likelihood

MLE free learning
- Do we really want MLE for generating good images?
	- For imperfect models high log-likelihood does not always mean good sample quality and vice versa (Theis et al., 2016)
 - Negative augmentation
	 - Tricky to do with likelihood models
 - Case 2
	 - Great test log-likelihood, poor samples.
	 - Is having one perfect image from 100 is really bad?
	 - Use ensembles to deal with 0.99 of noise from model
 - Case 3
	 - Great samples, poor log-likelihood

Distance measure
- Try to change distance measure for comparing two distribution, do not use D_KL
- Comparing distribution via samples
	- [[Two-sample test]]
	- hand-crafted statistics
	- comparing mean of two groups
	- likelihood free
- Key idea: Learn statistic
	- Discriminator
	- Learn Two-sample statistic
	- Test statistic: -loss of the classifier
	- Classification problem
	- Again likelihood?
		- But this classifier is also use cross-entropy or D_KL.
		- We use likelihood of the classifier, not generative model.
		- Likelihood over binary variable, not input image x

Generator
- Looks like a flow model, but the mapping shouldn't be invertible

Generator architecture restrictions
- https://chatgpt.com/c/675d86f1-8008-8000-ab6f-f5fe0b5856e7
- GAN
	- Now we don't any restrictions
- Autoregressive
	- Sequential process of generation
	- masked convolutions
	- masked attention
- Normalizing Flow
	- Invertible
	- Same number of dimensions for z and x
- VAE
	- Which restrictions VAE have?
	- Latent variable

Loss
- Jensen-Shannon Divergence (JSD)
	- D_KL between distribution and mixture of two distributions ? x2
	- Symmetric KL Divergence
- Under ideal conditions (optimal discriminator) this is equivalent to choosing for d(p_data, p_model) JSD instead of KLD


Problems
- Unstable optimization
	- Do not know when to stop, as with MLE when you can see that loss no longer improves
	- Why?
- Mode collapse
	- Focus on a few types of data points (in training set?) and completely stop generating others
	- few types of data points - they called modes
	- How to protect generator from memorizing the training set?
		- It does not have the access to it!
- Evaluation


GAN
- likelihood-free training

## f-GAN

f-Divergences
- Broader class of divergences

Fenchel conjugate
- https://chatgpt.com/c/675da715-8e20-8000-bdec-ea26fcaae9e8
- convex conjugate
- best linear approximation
- duality
- [[Convex Optimization]]

supremum
- https://en.wikipedia.org/wiki/Infimum_and_supremum
- supremum = least upper bound

f-GAN
- minimizing lower bound of f-Divergence

Why it's better than DKL?
- DKL == Compression
- Might not be what we want

## Wasserstein GAN

Support of a distribution: set of points that have non-zero probability

Problems with f-Divergence
- Arises when p and q have disjoint supports
- No signal from training in this case since Divergence always equal to some constant

Wasserstein distance
- Earth-Mover distance

Lipschitz constant
- TODO
- Lipschitz constant of f(x) is 1


## Inferring latent representation

Solution 1
- Use features from discriminator

Solution 2
- How to use generator?
- BiGAN
	- Encoder is deterministic mapping unlike in VAE


# Lecture 11, 12 - Energy Based Models

[[Energy-Based Models (EBMs)]]

DM
- EBM is closely related to DM
- As well as NF and VAE

EBM
- allows flexible model architectures
- stable training
- likelihood based
- inspired by statistical physics

Probability distribution p(x)
- Parameterizing probability distribution p(x)
- Not an arbitrary functions
- 2 restrictions
	- non-negative
		- Easy to satisfy
	- sum to 1
		- Trickier to satisfy
		- Intuition why we need
			- Total volume is fixed, increasing one p(x_1) we are sure that probability will decrease for all other xs
		- How this property achieved in other models?
			- NF - clear
			- VAE - ?
			- Autoregressive - ?
			- TODO
- Partition function - the integral of all space, all the volume
- Choose our function to be able to compute this partition function analytically
	- Example: [[Normal Distribution]]
	- Exponential family
	- [[Ветров. Введение в вероятностный язык построения моделей машинного обучения]]

Likelihood-based learning
- Autoregressive
- Latent Variables
- Why it's normalized by construction?
- Why VAE not in this list?
	- TODO

EBM
- Why exponent? Not x^2 ?
- vs. Softmax
	- Partition function can be computed exactly and easily
	- Learning just p(y|x)
- In general case the partition function is very hard to compute, if we model p(x)

Application of EBM
- When some task do not requires partition function
- Compare two data samples
	- Classification
	- Anomaly Detection
	- Denoising
- Ising Model
	- https://chatgpt.com/c/67654fca-9eb4-8000-bddd-94f379a58b3d

Product of Experts
- TODO

Restricted Boltzmann machine (RBM)
- TODO

## Training

Training intuition
- Just increasing unnormalized log-probability do not guarantee that this example becomes more likely
- Increase probability of train datapoint and decrease probability of everything else

Contrastive Divergence Algorithm
- Monte-Carlo estimate for partition function using negative samples
- Negative samples - samples from the model


## Sampling

How to get samples?

We need samples during training as well, not only during inference

Markov Chain Monte Carlo (MCMC)
- Use the fact we can do comparison
- Metropolis-Hastings Markov chain Monte Carlo
	- ?
- accept
- reject - we can also accept in this case with some probability which depends on how bad the new version of the image are
- Works in theory, in practice need a large number of steps to converge
- Convergence slows down exponentially with increasing dimensions

Unadjusted Langevin MCMC
- Special case of above
- Noisy gradient ascent
- You do not need accept or reject
- Unadjusted
	- TODO
- Convergence is still slow
- Very expensive training since this sampling is required on each step

Training without sampling
- Score matching
- Noise Contrastive Estimation
- Adversarial training

## Score matching

Score function
- Gradient of log probability with respect to x, not theta
- Function of x, and theta?! (see example with gaussian where theta is just two numbers - mean and variance)
	- parameterized by theta
- Why gradient of log?
	- log is monotonic, so gradient points to the same direction(and with the same magnitude) as for original PDF?
	- math is easy for EBM
- Alternative interpretation of PDF as a vector field, instead of scalar field
- Equivalent in Physics: electric field and potential
- Computationally it might my more handy to use one vs. another
	- Potential is more handy in Physics!
		- A
	- Field is more handy in DL

Score matching
- Fisher divergence
	- Difference between gradients of two distributions
	- If two distribution p and q are similar, they also should have similar gradients
- We do not know the gradient of the true data distribution
- Integration by parts
- Hessian is expensive
- Intuition of the loss function
	- Finding flat regions where gradient vectors are small
	- Hessian to ensure it's maximum, not minimum
- Q
	- When learning will we compute the gradient for gradient?
		- First - gradient wrt x
		- Second - gradient wrt theta, for updating params

Recap
- KL Divergence == MLE
	- Contrastive divergence
	- need to sample from model for training
- Fisher Divergence == Score matching
	- no need to sample


## NCE

Noise Contrastive Estimation (NCE)
- Noise distribution is fixed
- Training discriminator
- Optimal discriminator which use EBM inside
- Partition function as learnable parameter
	- Just a scaler?
	- Why we didn't do this earlier?

NCE vs. GAN

Flow Contrastive Estimation
- Use NF for generating noise as close to the true data distribution as we can to make work of discriminator harder
- Looks a lot like a GAN


# Lecture 13 - Score Based Models

Is score matching limited to EBMs?
- NO

Model the score function
- Instead of modeling the energy, we are directly model the score function.
- Broader family of models.
	- The gradient you are modeling might be not ...
		- the gradient of a scalar function
		- conservative vector field
	- There might not be an underlying scalar function
- Output of NN now is a gradient vector, not scalar?
- R^d -> R^d 


How to compare two vector fields?
- Fisher divergence?!
	- Average Euclidian distance over the all space


Jacobian -> Hessian
- https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant
- https://en.wikipedia.org/wiki/Hessian_matrix
- [[Hessian]]


Score matching is not scalable
- Trace of the Jacobian is the hard part
- O(d) backprop, where d is the number of dimension
	- TODO
	- We need to do several different backprop passes?

Denoising score matching
- Vincent, 2011
- Estimate score of the noised version of the distribution
	- noise-perturbed data distribution
	- q(x') - very complex
- Perturbation/distribution kernel
	- q(x'|x) - simple Gaussian
- Stein unbiased risk estimator
- Score matching reduces to denoising
- Loss formula
	- Why plus?
- Pros
	- Scalable
- Cons
	- Estimates scores of noise-perturbed data
		- How it affect the quality of samples?
		- We can generate only noisy samples! More about this later...
- Q:
	- Is the gradient of the disturbed image should be similar to the noise?
		- Yes
		- But why, if we estimates the gradient of the noise-perturbed data?
		- Tweedie's formula
	- Why we can't use the same model if we train it to predict the noise?
		- [chatgpt](https://chatgpt.com/g/g-p-676b182bc0e8819196b1f598b592448e-generative-models/c/676b1a2b-3ea8-8000-80ea-a24a636f3d09)
		- Получается что даже умея предсказывать тот шум который был добавлен мы не можем получить сэмпл из исходного распределения используя Langevin dynamic. 
		- Что если просто вычитать его из изображения?
			- Это тоже самое что и следовать градиенту
		- Ответ в том что в любом случае мы знаем только направление? Мы не учитываем длину вектора шума, когда обучаем модель?

Sliced score matching
- Comparing projections of the vectors instead of comparing them fully
- Random projections
- Pros
	- Estimates true data density

Sampling
- Langevin MCMC

Pitfalls
- Data lies in manifolds
	- Score function might be not well defined
	- TODO
- Challenges in low data density regions
- Mixing of Langevin dynamic
	- Example with mixture of two distributions
	- Modes of data distribution


# Lecture 14

Connection between score-based models and [[Diffusion Models]]

The solution to all above pitfalls: Gaussian perturbation

How much noise to add?
- Tradeoff
	- Add as little noise as possible to estimate something close to the real data
	- The more noise you add the better estimation becomes

Solution
- Using multiple noise scales
- multiple noise scales -> Annealed Langevin dynamic
- Noise conditional score networks (NCSN)

Sliced score matching for noise-perturbed data?
- TODO

Loss
- jointly solve many problems for different noise levels
- weighting factor

Choosing noise scales
- max - maximum pairwise distance between datapoints
- min - samples should looks like clean images

Infinite number of noise levels
- See picture from the slide!
- What represent the path of the image on this picture?
	- Sequence of random variables each drawn from their own distribution
	- Stochastic process
	- SDE
- Generation via reversing stochastic process
	- Similar on solving SDE by numerical methods
	- Euler-Maruyama
	- Same as Langevin dynamic
	- Now we can use a lot of advanced numerical methods for solving SDE for our task of image generation
- SDE
	- Forward SDE
	- Reverse SDE
		- Score function
		- What this score function mean? Previously we had many score function for each noise level. Same thing here?
- Training
	- Estimate the score function as we did before, but now t can take infinite number of values

Converting SDE to ODE
ODE and NF

# LAST