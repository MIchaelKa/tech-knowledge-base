
Convolutional Neural Networks for Visual Recognition

# External

Moved
https://www.notion.so/cs231n-f5cdcd5f61704e05b52f3f1c56641385

Main link
[http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)

Winter 2016
[http://cs231n.stanford.edu/2016/](http://cs231n.stanford.edu/2016/)
[https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)

2017
[https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)


# Links

[[Training Neural Networks (TNN)]]


# 5. Training Neural Networks

Lecture 5
2016 video
[https://youtu.be/gYpoJMlgyXA](https://youtu.be/gYpoJMlgyXA)

https://cs231n.github.io/neural-networks-2

[[Activation Functions]]

Data Preprocessing
- [[Data Normalization]]
- Center data:
	- Subtract the mean image ((32x32x3) array)
	- Subtract per-channel mean (3 numbers)
		- subtracting the mean across every individual _feature_ in the data
- PCA and Whitening
	- [[PCA]]
	- [[Singular Value Decomposition (SVD)]]
	- To decorrelate the data, we project the original (but zero-centered) data into the eigenbasis
	- Whitening
		- takes the data in the eigenbasis and divides every dimension by the eigenvalue to normalize the scale
	- Example with CIFAR-10
- Data leak
	- preprocessing statistics must only be computed on the training data, and then applied to the validation / test data

[[Weight Initialization]]

[[Regularization]]
- L2
	- heavily penalizing peaky weight vectors and preferring diffuse weight vectors
- Max norm constraints
	- Similar to [[Gradient Clipping]]

Loss functions
- [[Cross Entropy Loss (CEL)]]
- Problem: Large number of classes
	- Hierarchical Softmax
- Attribute classification
	- multi-label classifier
	- [[Classification]]
- Regression
	- L2 loss is much harder to optimize than a more stable loss such as Softmax
- Structured prediction
	- labels can be arbitrary structures such as graphs, trees, or other complex objects



# 6. Training Neural Networks

https://cs231n.github.io/neural-networks-3

This section is devoted to the dynamics, or in other words, the process of learning the parameters and finding good hyperparameters.

Gradient Checks
- comparing the analytic gradient to the numerical gradient

Before learning: sanity checks
- Look for correct loss at chance performance.
- Overfit a tiny subset of data.

Babysitting the learning process
- [[Training Neural Networks (TNN)]]
- Loss
- Train/Val accuracy
- Track the ratio of weight updates
	- update magnitudes to the value magnitudes
		- updates, not gradient, but $alpha*gradient$
	- [[Gradient Clipping]]
- Activation / Gradient distributions per layer
	- [[Histogram]]
- First-layer Visualizations
	- Does it works for 3x3 kernel sizes?

[[Parameter Updates]]

[[Learning Rate Decays]]

Hyperparameter Optimization
- hyperparameter search
- Master and workers
- Log space
	- Sample from log space
	- LR and regularization acts multiplicatively on the dynamics of your backprop
- Random search >> Grid search
- Coarse to fine
- BO
	- [[Active Learning]]


Model ensembles
- [[Ensembles]]
- Averaging checkpoints of a single model
- Running average parameter vector
```
x += learning_rate * dx
x_test = 0.995 * x_test + 0.005 * x
```


Transfer Learning
- With bigger dataset - train more layers
- Freezing layers
	- Sort of preprocessing step: extract features on the whole dataset and save it on disk. Train classifier on top of the cached cnn features.
- Finetuning
	- Train all layers but use lower learning rate for updating whole network: 1/10 of original LR
- Multi-stage approach
	- Let the last FC layer converge first. Its random initial weights will result in a large gradient that can mess up previous layers
	- https://youtu.be/pA4BsUK3oP4?t=1108


# 7. Convolutional Neural Networks

[[CNN]]

https://cs231n.github.io/convolutional-networks

https://www.notion.so/cs231n-7-Convolutional-Neural-Networks-f480c40c72c2496a9a20afe2ee4f1abc

## Discussion

Activations maps on deeper layers, are they too different from each other? Is it make sense to have different filter sizes for different activation maps?

## Overview

The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner

The asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in space (along width and height), but always full along the entire depth of the input volume.
- Can we change this?
	- Groups
	- [[Separable Convolution]]

Padding
- Zero-padding
	- is it better to use an image mean value instead of zeros or mirror the image?
- Why use padding?
	- This actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly.

Parameter sharing scheme
- Note that sometimes the parameter sharing assumption may not make sense.
	- input are faces that have been centered in the image
	- relax the parameter sharing scheme, and instead simply call the layer a Locally-Connected Layer

[[Dilated Convolution]]

Pooling Layer
- Reduce spatial size to reduce the amount of parameters and computation
- Controls overfitting
- Most common: 2x2 filter with stride 2
	- rejects 75% of activations

Number of parameters
- $K*K*Ci*C_o + C_o$

FC->CONV conversion
- allows us to “slide” the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass
- vs. FCN

Computational Considerations
- A clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below.
- [[ML Optimization (MLO)]]

Local Response Normalisation
- Not used any more, use BN or Dropout instead.
- VGG-
- GoogLeNet+

## Architectures

TODO

[[ResNet]]
[[Identity Mappings in Deep Residual Networks]]

Wide Residual Networks
- More channels
- Increasing width instead of depth. More computationally efficient.

ResNeXt
- Aggregated residual transformation for DNN
- Connected to inception module.
- Using idea of width from wide networks.

DenseNet

[[SENet]]

Stochastic depth

Network In Network
- old?


# 13. Generative models

[[Unsupervised Learning (UL)]]
[[Generative Models]]


Notion
- https://www.notion.so/Generative-Models-3a6cad46759e474da25b4387eddcb51f

Links
- https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv

Examples of unsupervised learning
- Clustering
- Dimentionality reduction
- Density estimation
- Feature Learning

UL
- Training data is cheap

Generative models addresses density estimation, a core problem in UL.


Why we need Generative Models
- Применение
- Super-resolution

Taxonomy
- Explicit
	- Explicit density estimation
	- Tractable density
		-  PixelRNN, PixelCNN
	- Approximate density
		- VAE
- Implicit
	- Implicit density estimation, produce samples w/o explicitly defining $P_{model}(x)$
	- GAN
- Where is [[Diffusion Models]] ?


PixelRNN and PixelCNN
- Fully visible belief networks (FVBN)
- Likelihood of an image
	- Use chain rule to decompose likelihood
	- We need to define ordering of previous pixels
- PixelRNN
	- Dependency on previous pixels modeled using RNN
	- Cons
		- Sequential generation is slow
- PixelCNN
	- Dependency on previous pixels modeled using CNN
		- Masked convolution
	- Training faster then PixelRNN, we can parallelize convolutions
	- Cons
		- Generation is still slow
- Loss
	- Softmax loss at each pixel value (0-255)
	- vs. regression
	- SSL?


VAE
- [[Variational Autoencoder (VAE)]]
- PixelCNN
	- PixelCNN defines tractable density functions.
	- We can directly optimize likelihood of the training data.
- VAE define intractable density functions with latent z.
- Using L2 loss for likelihood?


# LAST