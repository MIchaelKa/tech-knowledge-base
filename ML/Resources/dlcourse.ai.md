
# External

Notion
- https://www.notion.so/dlcourse-ai-f2572e55863b4f94b2090ed5bd3b03cb

Assignments
- https://drive.google.com/drive/u/1/folders/1gb6dcQw5smPh3w7r_QSpztnO6WI-6t_d


# Overview

https://dlcourse.ai/
Deep Learning на пальцах
2019


#  13 - Reinforcement Learning

[[Reinforcement Learning (RL)]]

Links
- https://karpathy.github.io/2016/05/31/rl/
- David Silver (фундаментальный курс)


policy function
p(a|s)

value function
v(s)

Q-function
Q(s,a)

мы будет приближать эти функции нейросетями
Deep RL

В чем отличие от обычной системы обучения?
- нет лейблов

Policy Gradients на примере пинпонга
- Трюк: передавать нейросети разницу между текущим и прошлым кадром
- Большой массив данных и слабый сигнал в виде одного числа
	- Эпизоды из 100 ходов
	- Почему сигнал не от каждого кадра?
		- Нужны лейблы для каждого кадра
- Кто играет за второго игрока?
	- простая эвристика которая движется по направлению к шарику
	- AI
- Делаем насколько эпизодов с одной и той же версией сети
	- Rollout
- Составляем датасет.
	- Вытягиваем ролауты друг за другом?
- Как составляем батчи?
	- Рандомно перемешиваем между ролаутами?
- Policy Gradients Loss
	- очень похож на [[Cross Entropy Loss (CEL)]]
	- логарифм вероятности, а в качестве мультиплайера реворд
- Делаем одну эпоху
- REPEAT

Policy Gradients
- похож на обучение с учителем
- вместо лейблов - результат эпизода игры
- получаем градиент одновременно для всех действий в эпизоде в зависимости от результата эпизода
- награды для всех шагов прогона одинаковые: +1 или -1 в зависимости от финального результата эпизода

Вопросы
- Как обучаться если выигрыш отдален?

Discounted rewards
- discount - сбрасывать со счетов
- вычитаем бейзлайн для реворда
	- REINFORCE 92
- нет смысла с простым сетапом реварда выше?
- как найти хороший бейзлайн?

Actor-Critic
- A2C
- как найти хороший бейзлайн?
	- обучаем value function

## DQN

Deep Q-Learning
- [[Q-Learning]]
- DeepMind
- DQN

Q-function
Q(s,a)
- выдает ожидаемый реворд
- чем это отличается от value function ?

Bellman equation
- С помощью этого уравнения можно итеративно приближать Q-function

Приближение Q-function
- С помощью Q-function можно посмотреть на шаг вперед
- Bellman equation

Преимущества (перед PG)
- Дает возможность обучаться на данных которые были засемплированы старой версией (сети?)

Вопрос (из чата)
- Можно ли в функцию Беллмана добавить больше одного шага?